{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24066,"status":"ok","timestamp":1669096411627,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"8Mfx5v8nlnQb","outputId":"89c25ba1-336c-4f5a-c8a8-3fb77cee1fc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/DL_mini_project\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%mkdir -p /content/drive/MyDrive/DL_mini_project\n","%cd /content/drive/MyDrive/DL_mini_project\n","%mkdir -p ./data\n","%mkdir -p ./checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4544,"status":"ok","timestamp":1669096418654,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"HNZ7ySQJecjW","outputId":"984e1035-4182-430e-d4be-0077fcceed1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.1\n"]}],"source":["!pip install torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"giFTZ4d9eYcY"},"outputs":[],"source":["from torchinfo import summary\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"vO0Z5rLx2R_6"},"source":["# Define Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTLIjOqC7ttl"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# TODO: change the hidden layer sizes: conv layer kernels, linear layer width, etc.\n","#       make sure that the dimensions between the blocks are consistent \n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion *\n","                               planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        #self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(256*block.expansion, num_classes)\n","        # avg pool\n","        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        #out = self.layer4(out)\n","        out = self.avgpool(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"q26mwPqa2W2G"},"source":["#Custom Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_G5odk9PqfrL"},"outputs":[],"source":["\n","# TODO: test to see how different combinations of blocks changes performance\n","#       e.g. How is [2, 4, 2, 2]? Try to figure out a combination that has the \n","#       number of parameters closest to 5M\n","\n","def ResNet_custom(name=\"ResNet18\"):\n","    name = name.lower()\n","    if name == \"model_1_1\":\n","        return ResNet(BasicBlock, [3, 5, 3])\n","    elif name == \"model_1_2\":\n","        return ResNet(BasicBlock, [3, 5, 3])\n","    elif name == \"model_1_3\":\n","        return ResNet(BasicBlock, [3, 5, 3])\n","    elif name == \"model_1_4\":\n","        return ResNet(BasicBlock, [3, 5, 3])\n","    elif name == \"model_2_1\":\n","        return ResNet(Bottleneck, [3, 3, 3])\n","    elif name == \"model_2_2\":\n","        return ResNet(Bottleneck, [3, 3, 3])\n","    elif name == \"model_2_3\":\n","        return ResNet(Bottleneck, [3, 3, 3])\n","    elif name == \"model_2_4\":\n","        return ResNet(Bottleneck, [3, 3, 3])\n","\n","\n","# def test():\n","#     net = ResNet_custom(name=\"ResNet18\")\n","#     y = net(torch.randn(1, 3, 32, 32))\n","#     print(y.size())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9436,"status":"ok","timestamp":1669096442439,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"TeZbiBNieqa_","outputId":"104657fd-e8b4-4c28-8754-6f9690874d22"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","ResNet                                   [10, 10]                  --\n","├─Conv2d: 1-1                            [10, 64, 32, 32]          1,728\n","├─BatchNorm2d: 1-2                       [10, 64, 32, 32]          128\n","├─Sequential: 1-3                        [10, 64, 32, 32]          --\n","│    └─BasicBlock: 2-1                   [10, 64, 32, 32]          --\n","│    │    └─Conv2d: 3-1                  [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-2             [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-3                  [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-4             [10, 64, 32, 32]          128\n","│    │    └─Sequential: 3-5              [10, 64, 32, 32]          --\n","│    └─BasicBlock: 2-2                   [10, 64, 32, 32]          --\n","│    │    └─Conv2d: 3-6                  [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-7             [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-8                  [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-9             [10, 64, 32, 32]          128\n","│    │    └─Sequential: 3-10             [10, 64, 32, 32]          --\n","│    └─BasicBlock: 2-3                   [10, 64, 32, 32]          --\n","│    │    └─Conv2d: 3-11                 [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-12            [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-13                 [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-14            [10, 64, 32, 32]          128\n","│    │    └─Sequential: 3-15             [10, 64, 32, 32]          --\n","├─Sequential: 1-4                        [10, 128, 16, 16]         --\n","│    └─BasicBlock: 2-4                   [10, 128, 16, 16]         --\n","│    │    └─Conv2d: 3-16                 [10, 128, 16, 16]         73,728\n","│    │    └─BatchNorm2d: 3-17            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-18                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-19            [10, 128, 16, 16]         256\n","│    │    └─Sequential: 3-20             [10, 128, 16, 16]         8,448\n","│    └─BasicBlock: 2-5                   [10, 128, 16, 16]         --\n","│    │    └─Conv2d: 3-21                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-22            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-23                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-24            [10, 128, 16, 16]         256\n","│    │    └─Sequential: 3-25             [10, 128, 16, 16]         --\n","│    └─BasicBlock: 2-6                   [10, 128, 16, 16]         --\n","│    │    └─Conv2d: 3-26                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-27            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-28                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-29            [10, 128, 16, 16]         256\n","│    │    └─Sequential: 3-30             [10, 128, 16, 16]         --\n","│    └─BasicBlock: 2-7                   [10, 128, 16, 16]         --\n","│    │    └─Conv2d: 3-31                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-32            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-33                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-34            [10, 128, 16, 16]         256\n","│    │    └─Sequential: 3-35             [10, 128, 16, 16]         --\n","│    └─BasicBlock: 2-8                   [10, 128, 16, 16]         --\n","│    │    └─Conv2d: 3-36                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-37            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-38                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-39            [10, 128, 16, 16]         256\n","│    │    └─Sequential: 3-40             [10, 128, 16, 16]         --\n","├─Sequential: 1-5                        [10, 256, 8, 8]           --\n","│    └─BasicBlock: 2-9                   [10, 256, 8, 8]           --\n","│    │    └─Conv2d: 3-41                 [10, 256, 8, 8]           294,912\n","│    │    └─BatchNorm2d: 3-42            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-43                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-44            [10, 256, 8, 8]           512\n","│    │    └─Sequential: 3-45             [10, 256, 8, 8]           33,280\n","│    └─BasicBlock: 2-10                  [10, 256, 8, 8]           --\n","│    │    └─Conv2d: 3-46                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-47            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-48                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-49            [10, 256, 8, 8]           512\n","│    │    └─Sequential: 3-50             [10, 256, 8, 8]           --\n","│    └─BasicBlock: 2-11                  [10, 256, 8, 8]           --\n","│    │    └─Conv2d: 3-51                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-52            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-53                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-54            [10, 256, 8, 8]           512\n","│    │    └─Sequential: 3-55             [10, 256, 8, 8]           --\n","├─AdaptiveAvgPool2d: 1-6                 [10, 256, 1, 1]           --\n","├─Linear: 1-7                            [10, 10]                  2,570\n","==========================================================================================\n","Total params: 4,918,602\n","Trainable params: 4,918,602\n","Non-trainable params: 0\n","Total mult-adds (G): 7.99\n","==========================================================================================\n","Input size (MB): 0.12\n","Forward/backward pass size (MB): 149.42\n","Params size (MB): 19.67\n","Estimated Total Size (MB): 169.22\n","=========================================================================================="]},"metadata":{},"execution_count":8}],"source":["summary(ResNet_custom('model_1_3'), (10,3, 32, 32), depth=3)"]},{"cell_type":"markdown","metadata":{"id":"_pyJLJ-32y08"},"source":["# Helper Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqREt3HP8l8t"},"outputs":[],"source":["'''Some helper functions for PyTorch, including:\n","    - get_mean_and_std: calculate the mean and std value of dataset.\n","    - msr_init: net parameter initialization.\n","    - progress_bar: progress bar mimic xlua.progress.\n","'''\n","import os\n","import sys\n","import time\n","import math\n","\n","import torch.nn as nn\n","import torch.nn.init as init\n","\n","\n","def get_mean_and_std(dataset):\n","    '''Compute the mean and std value of dataset.'''\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n","    mean = torch.zeros(3)\n","    std = torch.zeros(3)\n","    print('==> Computing mean and std..')\n","    for inputs, targets in dataloader:\n","        for i in range(3):\n","            mean[i] += inputs[:,i,:,:].mean()\n","            std[i] += inputs[:,i,:,:].std()\n","    mean.div_(len(dataset))\n","    std.div_(len(dataset))\n","    return mean, std\n","\n","def init_params(net):\n","    '''Init layer parameters.'''\n","    for m in net.modules():\n","        if isinstance(m, nn.Conv2d):\n","            init.kaiming_normal(m.weight, mode='fan_out')\n","            if m.bias:\n","                init.constant(m.bias, 0)\n","        elif isinstance(m, nn.BatchNorm2d):\n","            init.constant(m.weight, 1)\n","            init.constant(m.bias, 0)\n","        elif isinstance(m, nn.Linear):\n","            init.normal(m.weight, std=1e-3)\n","            if m.bias:\n","                init.constant(m.bias, 0)\n","\n","\n","# _, term_width = os.popen('stty size', 'r').read().split()\n","term_width = 80\n","\n","TOTAL_BAR_LENGTH = 65.\n","last_time = time.time()\n","begin_time = last_time\n","def progress_bar(current, total, msg=None):\n","    global last_time, begin_time\n","    if current == 0:\n","        begin_time = time.time()  # Reset for new bar.\n","\n","    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n","    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n","\n","    sys.stdout.write(' [')\n","    for i in range(cur_len):\n","        sys.stdout.write('=')\n","    sys.stdout.write('>')\n","    for i in range(rest_len):\n","        sys.stdout.write('.')\n","    sys.stdout.write(']')\n","\n","    cur_time = time.time()\n","    step_time = cur_time - last_time\n","    last_time = cur_time\n","    tot_time = cur_time - begin_time\n","\n","    L = []\n","    L.append('  Step: %s' % format_time(step_time))\n","    L.append(' | Tot: %s' % format_time(tot_time))\n","    if msg:\n","        L.append(' | ' + msg)\n","\n","    msg = ''.join(L)\n","    sys.stdout.write(msg)\n","    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n","        sys.stdout.write(' ')\n","\n","    # Go back to the center of the bar.\n","    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n","        sys.stdout.write('\\b')\n","    sys.stdout.write(' %d/%d ' % (current+1, total))\n","\n","    if current < total-1:\n","        sys.stdout.write('\\r')\n","    else:\n","        sys.stdout.write('\\n')\n","    sys.stdout.flush()\n","\n","def format_time(seconds):\n","    days = int(seconds / 3600/24)\n","    seconds = seconds - days*3600*24\n","    hours = int(seconds / 3600)\n","    seconds = seconds - hours*3600\n","    minutes = int(seconds / 60)\n","    seconds = seconds - minutes*60\n","    secondsf = int(seconds)\n","    seconds = seconds - secondsf\n","    millis = int(seconds*1000)\n","\n","    f = ''\n","    i = 1\n","    if days > 0:\n","        f += str(days) + 'D'\n","        i += 1\n","    if hours > 0 and i <= 2:\n","        f += str(hours) + 'h'\n","        i += 1\n","    if minutes > 0 and i <= 2:\n","        f += str(minutes) + 'm'\n","        i += 1\n","    if secondsf > 0 and i <= 2:\n","        f += str(secondsf) + 's'\n","        i += 1\n","    if millis > 0 and i <= 2:\n","        f += str(millis) + 'ms'\n","        i += 1\n","    if f == '':\n","        f = '0ms'\n","    return f\n","\n","import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"markdown","metadata":{"id":"g1DM1fUb32dw"},"source":["# Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13712,"status":"ok","timestamp":1669096456149,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"p8E0Tcuu7yPy","outputId":"50161730-3e54-4234-9f77-510ac01d0e4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["'''Train CIFAR10 with PyTorch.'''\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","#import argparse\n","\n","\n","\n","'''\n","parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n","parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n","parser.add_argument('--resume', '-r', action='store_true',\n","                    help='resume from checkpoint')\n","args = parser.parse_args()\n","'''\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","best_acc = 0  # best test accuracy\n","start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","\n","# Data\n","print('==> Preparing data..')\n","\n","# TODO: Change data augmentation on the training set: e.g. CenterCrop vs RandomCrop \n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(\n","    trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(\n","    testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ix7L5xWH-xJt"},"source":["# Model_1_1 (with constant lr=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":485,"status":"ok","timestamp":1668719680996,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"m_4DjKLG-vXf","outputId":"bb7c1a31-1de7-438f-d5c2-adc56e1cda90"},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","ResNet                                   [10, 10]                  --\n","├─Conv2d: 1-1                            [10, 64, 32, 32]          1,728\n","├─BatchNorm2d: 1-2                       [10, 64, 32, 32]          128\n","├─Sequential: 1-3                        [10, 64, 32, 32]          --\n","│    └─BasicBlock: 2-1                   [10, 64, 32, 32]          --\n","│    │    └─Conv2d: 3-1                  [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-2             [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-3                  [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-4             [10, 64, 32, 32]          128\n","│    │    └─Sequential: 3-5              [10, 64, 32, 32]          --\n","│    └─BasicBlock: 2-2                   [10, 64, 32, 32]          --\n","│    │    └─Conv2d: 3-6                  [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-7             [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-8                  [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-9             [10, 64, 32, 32]          128\n","│    │    └─Sequential: 3-10             [10, 64, 32, 32]          --\n","│    └─BasicBlock: 2-3                   [10, 64, 32, 32]          --\n","│    │    └─Conv2d: 3-11                 [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-12            [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-13                 [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-14            [10, 64, 32, 32]          128\n","│    │    └─Sequential: 3-15             [10, 64, 32, 32]          --\n","├─Sequential: 1-4                        [10, 128, 16, 16]         --\n","│    └─BasicBlock: 2-4                   [10, 128, 16, 16]         --\n","│    │    └─Conv2d: 3-16                 [10, 128, 16, 16]         73,728\n","│    │    └─BatchNorm2d: 3-17            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-18                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-19            [10, 128, 16, 16]         256\n","│    │    └─Sequential: 3-20             [10, 128, 16, 16]         8,448\n","│    └─BasicBlock: 2-5                   [10, 128, 16, 16]         --\n","│    │    └─Conv2d: 3-21                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-22            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-23                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-24            [10, 128, 16, 16]         256\n","│    │    └─Sequential: 3-25             [10, 128, 16, 16]         --\n","│    └─BasicBlock: 2-6                   [10, 128, 16, 16]         --\n","│    │    └─Conv2d: 3-26                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-27            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-28                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-29            [10, 128, 16, 16]         256\n","│    │    └─Sequential: 3-30             [10, 128, 16, 16]         --\n","│    └─BasicBlock: 2-7                   [10, 128, 16, 16]         --\n","│    │    └─Conv2d: 3-31                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-32            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-33                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-34            [10, 128, 16, 16]         256\n","│    │    └─Sequential: 3-35             [10, 128, 16, 16]         --\n","│    └─BasicBlock: 2-8                   [10, 128, 16, 16]         --\n","│    │    └─Conv2d: 3-36                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-37            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-38                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-39            [10, 128, 16, 16]         256\n","│    │    └─Sequential: 3-40             [10, 128, 16, 16]         --\n","├─Sequential: 1-5                        [10, 256, 8, 8]           --\n","│    └─BasicBlock: 2-9                   [10, 256, 8, 8]           --\n","│    │    └─Conv2d: 3-41                 [10, 256, 8, 8]           294,912\n","│    │    └─BatchNorm2d: 3-42            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-43                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-44            [10, 256, 8, 8]           512\n","│    │    └─Sequential: 3-45             [10, 256, 8, 8]           33,280\n","│    └─BasicBlock: 2-10                  [10, 256, 8, 8]           --\n","│    │    └─Conv2d: 3-46                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-47            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-48                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-49            [10, 256, 8, 8]           512\n","│    │    └─Sequential: 3-50             [10, 256, 8, 8]           --\n","│    └─BasicBlock: 2-11                  [10, 256, 8, 8]           --\n","│    │    └─Conv2d: 3-51                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-52            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-53                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-54            [10, 256, 8, 8]           512\n","│    │    └─Sequential: 3-55             [10, 256, 8, 8]           --\n","├─AdaptiveAvgPool2d: 1-6                 [10, 256, 1, 1]           --\n","├─Linear: 1-7                            [10, 10]                  2,570\n","==========================================================================================\n","Total params: 4,918,602\n","Trainable params: 4,918,602\n","Non-trainable params: 0\n","Total mult-adds (G): 7.99\n","==========================================================================================\n","Input size (MB): 0.12\n","Forward/backward pass size (MB): 149.42\n","Params size (MB): 19.67\n","Estimated Total Size (MB): 169.22\n","=========================================================================================="]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["summary(ResNet_custom('model_1_1'), (10,3, 32, 32), depth=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5364799,"status":"ok","timestamp":1668725074218,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"OQ8FRrmg-wkq","outputId":"a22edeec-37b8-4e91-e5bd-d55d41e71ba3"},"outputs":[{"name":"stdout","output_type":"stream","text":["==> Building model..\n","\n","Epoch: 0\n","390 391 Loss: 1.926 | Acc: 28.010% (14005/50000)\n","99 100 Loss: 1.611 | Acc: 38.640% (3864/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 1\n","390 391 Loss: 1.452 | Acc: 46.240% (23120/50000)\n","99 100 Loss: 1.383 | Acc: 49.980% (4998/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 2\n","390 391 Loss: 1.176 | Acc: 57.478% (28739/50000)\n","99 100 Loss: 1.161 | Acc: 58.840% (5884/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 3\n","390 391 Loss: 0.968 | Acc: 65.724% (32862/50000)\n","99 100 Loss: 0.904 | Acc: 68.640% (6864/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 4\n","390 391 Loss: 0.841 | Acc: 70.248% (35124/50000)\n","99 100 Loss: 1.004 | Acc: 65.580% (6558/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 5\n","390 391 Loss: 0.712 | Acc: 75.228% (37614/50000)\n","99 100 Loss: 1.013 | Acc: 68.090% (6809/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 6\n","390 391 Loss: 0.625 | Acc: 78.370% (39185/50000)\n","99 100 Loss: 0.761 | Acc: 74.800% (7480/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 7\n","390 391 Loss: 0.570 | Acc: 80.364% (40182/50000)\n","99 100 Loss: 0.909 | Acc: 70.610% (7061/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 8\n","390 391 Loss: 0.526 | Acc: 81.930% (40965/50000)\n","99 100 Loss: 0.619 | Acc: 78.940% (7894/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 9\n","390 391 Loss: 0.498 | Acc: 83.086% (41543/50000)\n","99 100 Loss: 0.705 | Acc: 76.530% (7653/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 10\n","390 391 Loss: 0.471 | Acc: 83.880% (41940/50000)\n","99 100 Loss: 0.948 | Acc: 71.030% (7103/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 11\n","390 391 Loss: 0.459 | Acc: 84.180% (42090/50000)\n","99 100 Loss: 0.766 | Acc: 75.960% (7596/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 12\n","390 391 Loss: 0.437 | Acc: 85.002% (42501/50000)\n","99 100 Loss: 0.549 | Acc: 81.800% (8180/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 13\n","390 391 Loss: 0.423 | Acc: 85.516% (42758/50000)\n","99 100 Loss: 0.547 | Acc: 81.700% (8170/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 14\n","390 391 Loss: 0.411 | Acc: 85.874% (42937/50000)\n","99 100 Loss: 0.522 | Acc: 82.740% (8274/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 15\n","390 391 Loss: 0.396 | Acc: 86.338% (43169/50000)\n","99 100 Loss: 0.599 | Acc: 80.190% (8019/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 16\n","390 391 Loss: 0.390 | Acc: 86.738% (43369/50000)\n","99 100 Loss: 0.697 | Acc: 77.010% (7701/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 17\n","390 391 Loss: 0.382 | Acc: 87.004% (43502/50000)\n","99 100 Loss: 0.794 | Acc: 75.280% (7528/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 18\n","390 391 Loss: 0.373 | Acc: 87.260% (43630/50000)\n","99 100 Loss: 0.626 | Acc: 79.900% (7990/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 19\n","390 391 Loss: 0.372 | Acc: 87.222% (43611/50000)\n","99 100 Loss: 0.602 | Acc: 80.960% (8096/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 20\n","390 391 Loss: 0.370 | Acc: 87.312% (43656/50000)\n","99 100 Loss: 1.123 | Acc: 70.470% (7047/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 21\n","390 391 Loss: 0.364 | Acc: 87.422% (43711/50000)\n","99 100 Loss: 0.716 | Acc: 78.870% (7887/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 22\n","390 391 Loss: 0.357 | Acc: 87.862% (43931/50000)\n","99 100 Loss: 0.646 | Acc: 80.540% (8054/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 23\n","390 391 Loss: 0.353 | Acc: 87.854% (43927/50000)\n","99 100 Loss: 0.517 | Acc: 83.110% (8311/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 24\n","390 391 Loss: 0.350 | Acc: 87.982% (43991/50000)\n","99 100 Loss: 0.529 | Acc: 82.580% (8258/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 25\n","390 391 Loss: 0.344 | Acc: 88.114% (44057/50000)\n","99 100 Loss: 0.528 | Acc: 82.600% (8260/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 26\n","390 391 Loss: 0.348 | Acc: 88.108% (44054/50000)\n","99 100 Loss: 0.517 | Acc: 83.630% (8363/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 27\n","390 391 Loss: 0.337 | Acc: 88.492% (44246/50000)\n","99 100 Loss: 0.583 | Acc: 81.640% (8164/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 28\n","390 391 Loss: 0.342 | Acc: 88.420% (44210/50000)\n","99 100 Loss: 0.618 | Acc: 81.540% (8154/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 29\n","390 391 Loss: 0.340 | Acc: 88.286% (44143/50000)\n","99 100 Loss: 0.581 | Acc: 81.000% (8100/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 30\n","390 391 Loss: 0.334 | Acc: 88.640% (44320/50000)\n","99 100 Loss: 0.520 | Acc: 83.470% (8347/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 31\n","390 391 Loss: 0.334 | Acc: 88.448% (44224/50000)\n","99 100 Loss: 0.903 | Acc: 72.600% (7260/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 32\n","390 391 Loss: 0.326 | Acc: 89.096% (44548/50000)\n","99 100 Loss: 0.554 | Acc: 81.810% (8181/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 33\n","390 391 Loss: 0.331 | Acc: 88.536% (44268/50000)\n","99 100 Loss: 0.650 | Acc: 80.090% (8009/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 34\n","390 391 Loss: 0.318 | Acc: 89.112% (44556/50000)\n","99 100 Loss: 0.430 | Acc: 85.960% (8596/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 35\n","390 391 Loss: 0.321 | Acc: 88.866% (44433/50000)\n","99 100 Loss: 0.487 | Acc: 84.450% (8445/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 36\n","390 391 Loss: 0.318 | Acc: 89.036% (44518/50000)\n","99 100 Loss: 0.615 | Acc: 80.730% (8073/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 37\n","390 391 Loss: 0.314 | Acc: 89.196% (44598/50000)\n","99 100 Loss: 0.715 | Acc: 78.650% (7865/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 38\n","390 391 Loss: 0.311 | Acc: 89.344% (44672/50000)\n","99 100 Loss: 0.474 | Acc: 83.930% (8393/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 39\n","390 391 Loss: 0.312 | Acc: 89.412% (44706/50000)\n","99 100 Loss: 0.590 | Acc: 80.700% (8070/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 40\n","390 391 Loss: 0.319 | Acc: 89.122% (44561/50000)\n","99 100 Loss: 0.522 | Acc: 82.430% (8243/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 41\n","390 391 Loss: 0.311 | Acc: 89.372% (44686/50000)\n","99 100 Loss: 0.656 | Acc: 80.530% (8053/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 42\n","390 391 Loss: 0.308 | Acc: 89.470% (44735/50000)\n","99 100 Loss: 0.628 | Acc: 79.770% (7977/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 43\n","390 391 Loss: 0.305 | Acc: 89.662% (44831/50000)\n","99 100 Loss: 0.430 | Acc: 85.820% (8582/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 44\n","390 391 Loss: 0.313 | Acc: 89.128% (44564/50000)\n","99 100 Loss: 0.601 | Acc: 81.240% (8124/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 45\n","390 391 Loss: 0.307 | Acc: 89.512% (44756/50000)\n","99 100 Loss: 0.423 | Acc: 85.550% (8555/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 46\n","390 391 Loss: 0.308 | Acc: 89.436% (44718/50000)\n","99 100 Loss: 0.449 | Acc: 85.270% (8527/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 47\n","390 391 Loss: 0.304 | Acc: 89.730% (44865/50000)\n","99 100 Loss: 0.411 | Acc: 86.470% (8647/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 48\n","390 391 Loss: 0.306 | Acc: 89.654% (44827/50000)\n","99 100 Loss: 0.525 | Acc: 83.290% (8329/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 49\n","390 391 Loss: 0.307 | Acc: 89.546% (44773/50000)\n","99 100 Loss: 0.560 | Acc: 82.970% (8297/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 50\n","390 391 Loss: 0.304 | Acc: 89.504% (44752/50000)\n","99 100 Loss: 0.514 | Acc: 83.180% (8318/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 51\n","390 391 Loss: 0.303 | Acc: 89.664% (44832/50000)\n","99 100 Loss: 0.617 | Acc: 80.120% (8012/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 52\n","390 391 Loss: 0.305 | Acc: 89.502% (44751/50000)\n","99 100 Loss: 0.434 | Acc: 84.710% (8471/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 53\n","390 391 Loss: 0.301 | Acc: 89.664% (44832/50000)\n","99 100 Loss: 0.453 | Acc: 85.020% (8502/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 54\n","390 391 Loss: 0.301 | Acc: 89.560% (44780/50000)\n","99 100 Loss: 0.602 | Acc: 81.430% (8143/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 55\n","390 391 Loss: 0.302 | Acc: 89.642% (44821/50000)\n","99 100 Loss: 0.417 | Acc: 86.110% (8611/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 56\n","390 391 Loss: 0.302 | Acc: 89.660% (44830/50000)\n","99 100 Loss: 0.375 | Acc: 87.690% (8769/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 57\n","390 391 Loss: 0.295 | Acc: 89.808% (44904/50000)\n","99 100 Loss: 0.712 | Acc: 78.230% (7823/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 58\n","390 391 Loss: 0.304 | Acc: 89.574% (44787/50000)\n","99 100 Loss: 0.444 | Acc: 85.630% (8563/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 59\n","390 391 Loss: 0.296 | Acc: 89.940% (44970/50000)\n","99 100 Loss: 0.494 | Acc: 84.510% (8451/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 60\n","390 391 Loss: 0.301 | Acc: 89.742% (44871/50000)\n","99 100 Loss: 0.536 | Acc: 82.680% (8268/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 61\n","390 391 Loss: 0.294 | Acc: 89.982% (44991/50000)\n","99 100 Loss: 0.450 | Acc: 85.690% (8569/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 62\n","390 391 Loss: 0.297 | Acc: 89.802% (44901/50000)\n","99 100 Loss: 0.442 | Acc: 85.930% (8593/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 63\n","390 391 Loss: 0.294 | Acc: 90.016% (45008/50000)\n","99 100 Loss: 0.430 | Acc: 85.760% (8576/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 64\n","390 391 Loss: 0.293 | Acc: 89.854% (44927/50000)\n","99 100 Loss: 0.526 | Acc: 83.470% (8347/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 65\n","390 391 Loss: 0.302 | Acc: 89.696% (44848/50000)\n","99 100 Loss: 0.756 | Acc: 77.630% (7763/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 66\n","390 391 Loss: 0.296 | Acc: 90.020% (45010/50000)\n","99 100 Loss: 0.438 | Acc: 85.480% (8548/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 67\n","390 391 Loss: 0.296 | Acc: 89.898% (44949/50000)\n","99 100 Loss: 0.543 | Acc: 82.810% (8281/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 68\n","390 391 Loss: 0.293 | Acc: 90.006% (45003/50000)\n","99 100 Loss: 0.469 | Acc: 84.880% (8488/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 69\n","390 391 Loss: 0.295 | Acc: 89.894% (44947/50000)\n","99 100 Loss: 0.473 | Acc: 84.340% (8434/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 70\n","390 391 Loss: 0.292 | Acc: 90.030% (45015/50000)\n","99 100 Loss: 0.529 | Acc: 82.980% (8298/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 71\n","390 391 Loss: 0.293 | Acc: 90.138% (45069/50000)\n","99 100 Loss: 0.484 | Acc: 84.790% (8479/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 72\n","390 391 Loss: 0.285 | Acc: 90.308% (45154/50000)\n","99 100 Loss: 0.437 | Acc: 85.850% (8585/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 73\n","390 391 Loss: 0.297 | Acc: 89.810% (44905/50000)\n","99 100 Loss: 0.668 | Acc: 80.880% (8088/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 74\n","390 391 Loss: 0.296 | Acc: 89.884% (44942/50000)\n","99 100 Loss: 0.480 | Acc: 84.110% (8411/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 75\n","390 391 Loss: 0.294 | Acc: 89.954% (44977/50000)\n","99 100 Loss: 0.454 | Acc: 85.190% (8519/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 76\n","390 391 Loss: 0.293 | Acc: 89.882% (44941/50000)\n","99 100 Loss: 0.442 | Acc: 85.550% (8555/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 77\n","390 391 Loss: 0.294 | Acc: 89.942% (44971/50000)\n","99 100 Loss: 0.428 | Acc: 86.010% (8601/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 78\n","390 391 Loss: 0.285 | Acc: 90.272% (45136/50000)\n","99 100 Loss: 0.708 | Acc: 78.820% (7882/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 79\n","390 391 Loss: 0.290 | Acc: 90.054% (45027/50000)\n","99 100 Loss: 0.422 | Acc: 86.640% (8664/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 80\n","390 391 Loss: 0.287 | Acc: 90.142% (45071/50000)\n","99 100 Loss: 0.494 | Acc: 83.940% (8394/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 81\n","390 391 Loss: 0.289 | Acc: 90.120% (45060/50000)\n","99 100 Loss: 0.528 | Acc: 83.200% (8320/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 82\n","390 391 Loss: 0.289 | Acc: 90.060% (45030/50000)\n","99 100 Loss: 0.468 | Acc: 84.380% (8438/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 83\n","390 391 Loss: 0.288 | Acc: 90.022% (45011/50000)\n","99 100 Loss: 0.636 | Acc: 80.540% (8054/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 84\n","390 391 Loss: 0.290 | Acc: 90.094% (45047/50000)\n","99 100 Loss: 0.435 | Acc: 85.460% (8546/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 85\n","390 391 Loss: 0.286 | Acc: 90.222% (45111/50000)\n","99 100 Loss: 0.390 | Acc: 87.230% (8723/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 86\n","390 391 Loss: 0.282 | Acc: 90.424% (45212/50000)\n","99 100 Loss: 0.678 | Acc: 80.890% (8089/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 87\n","390 391 Loss: 0.288 | Acc: 90.106% (45053/50000)\n","99 100 Loss: 0.370 | Acc: 87.910% (8791/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 88\n","390 391 Loss: 0.284 | Acc: 90.246% (45123/50000)\n","99 100 Loss: 0.783 | Acc: 77.940% (7794/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 89\n","390 391 Loss: 0.288 | Acc: 90.220% (45110/50000)\n","99 100 Loss: 0.475 | Acc: 84.370% (8437/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 90\n","390 391 Loss: 0.282 | Acc: 90.466% (45233/50000)\n","99 100 Loss: 0.539 | Acc: 83.000% (8300/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 91\n","390 391 Loss: 0.284 | Acc: 90.332% (45166/50000)\n","99 100 Loss: 0.586 | Acc: 81.670% (8167/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 92\n","390 391 Loss: 0.290 | Acc: 89.966% (44983/50000)\n","99 100 Loss: 0.421 | Acc: 86.280% (8628/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 93\n","390 391 Loss: 0.290 | Acc: 90.132% (45066/50000)\n","99 100 Loss: 0.540 | Acc: 82.920% (8292/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 94\n","390 391 Loss: 0.285 | Acc: 90.186% (45093/50000)\n","99 100 Loss: 0.450 | Acc: 85.040% (8504/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 95\n","390 391 Loss: 0.284 | Acc: 90.296% (45148/50000)\n","99 100 Loss: 0.522 | Acc: 83.020% (8302/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 96\n","390 391 Loss: 0.288 | Acc: 90.134% (45067/50000)\n","99 100 Loss: 0.517 | Acc: 83.460% (8346/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 97\n","390 391 Loss: 0.287 | Acc: 90.266% (45133/50000)\n","99 100 Loss: 0.429 | Acc: 86.510% (8651/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 98\n","390 391 Loss: 0.291 | Acc: 90.062% (45031/50000)\n","99 100 Loss: 0.527 | Acc: 82.510% (8251/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 99\n","390 391 Loss: 0.283 | Acc: 90.284% (45142/50000)\n","99 100 Loss: 0.530 | Acc: 83.890% (8389/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n"]}],"source":["# TODO: Change the model to your own selection\n","name = \"model_1_1\"\n","name = name.lower()\n","resume = None\n","# resume = f\"./checkpoint/{name}/001.pth\" # change the checkpoint name to the one desired\n","\n","# Model\n","print('==> Building model..')\n","net = ResNet_custom(name).to(device)\n","\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","if resume is not None:\n","    # Load checkpoint.\n","    print(f'==> Resuming from {resume}..')\n","    assert os.path.exists(resume), 'Error: no checkpoint found!'\n","    checkpoint = torch.load(resume)\n","    assert name == checkpoint['name'], 'Error: model does not match checkpoint!'\n","    net.load_state_dict(checkpoint['net'])\n","    best_acc = checkpoint['acc']\n","    start_epoch = checkpoint['epoch']\n","\n","# TODO: Loss\n","criterion = nn.CrossEntropyLoss()\n","# TODO: optimizer, SGD vs Adam, learning rate selection, etc...\n","optimizer = optim.SGD(net.parameters(), lr=0.1,\n","                      momentum=0.9, weight_decay=5e-4)\n","#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n","\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","    train_loss_list.append(train_loss/(batch_idx+1))\n","    train_ACC_list.append(100.*correct/total)\n","\n","\n","    print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","        test_loss_list.append(test_loss/(batch_idx+1))\n","        test_ACC_list.append(100.*correct/total)\n","\n","        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'name': name,\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir(f'checkpoint/{name}'):\n","            os.mkdir(f'checkpoint/{name}')\n","        torch.save(state, f'./checkpoint/{name}/{epoch:03}.pth')\n","        best_acc = acc\n","\n","train_loss_list = []\n","test_loss_list = []\n","train_ACC_list = []\n","test_ACC_list = []\n","lr_list = []\n","\n","# TODO: decide when the training should stop\n","for epoch in range(start_epoch, start_epoch+100):\n","    start_time = time.time()\n","\n","    train(epoch)\n","    test(epoch)\n","\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(\"Learning Rate: %f | Epoch Time: %i m %i s\" % (optimizer.param_groups[0]['lr'], epoch_mins,epoch_secs))\n","\n","    lr_list.append(optimizer.param_groups[0]['lr'])\n","\n","    #scheduler.step()\n","\n","    # break # for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lv0ofgUAU4St"},"outputs":[],"source":["result = pd.DataFrame(np.array([train_loss_list, test_loss_list,\n","                       train_ACC_list, test_ACC_list,\n","                       lr_list]).T, columns=['train_loss','test_loss','train_ACC','test_ACC','lr'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F279FZatUA3R"},"outputs":[],"source":["if not os.path.isdir(f'checkpoint/{name}'):\n","    os.mkdir(f'checkpoint/{name}')\n","result.to_csv(f'./checkpoint/{name}/result.csv')"]},{"cell_type":"markdown","metadata":{"id":"5iYiIeEXrG43"},"source":["# Model_1_2 (Decay lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5355896,"status":"ok","timestamp":1668730430442,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"LWTgySBNq7aZ","outputId":"343cd034-7d86-405e-ea32-a2107a62850c"},"outputs":[{"name":"stdout","output_type":"stream","text":["==> Building model..\n","\n","Epoch: 0\n","390 391 Loss: 1.899 | Acc: 30.262% (15131/50000)\n","99 100 Loss: 1.566 | Acc: 41.500% (4150/10000)\n","Learning Rate: 0.100000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 1\n","390 391 Loss: 1.413 | Acc: 47.904% (23952/50000)\n","99 100 Loss: 1.322 | Acc: 52.310% (5231/10000)\n","Learning Rate: 0.099975 | Epoch Time: 0 m 53 s\n","\n","Epoch: 2\n","390 391 Loss: 1.119 | Acc: 59.936% (29968/50000)\n","99 100 Loss: 1.091 | Acc: 61.510% (6151/10000)\n","Learning Rate: 0.099901 | Epoch Time: 0 m 53 s\n","\n","Epoch: 3\n","390 391 Loss: 0.908 | Acc: 68.094% (34047/50000)\n","99 100 Loss: 0.934 | Acc: 66.430% (6643/10000)\n","Learning Rate: 0.099778 | Epoch Time: 0 m 54 s\n","\n","Epoch: 4\n","390 391 Loss: 0.740 | Acc: 74.032% (37016/50000)\n","99 100 Loss: 0.765 | Acc: 74.410% (7441/10000)\n","Learning Rate: 0.099606 | Epoch Time: 0 m 53 s\n","\n","Epoch: 5\n","390 391 Loss: 0.635 | Acc: 78.076% (39038/50000)\n","99 100 Loss: 0.972 | Acc: 69.260% (6926/10000)\n","Learning Rate: 0.099384 | Epoch Time: 0 m 54 s\n","\n","Epoch: 6\n","390 391 Loss: 0.575 | Acc: 80.104% (40052/50000)\n","99 100 Loss: 1.044 | Acc: 69.070% (6907/10000)\n","Learning Rate: 0.099114 | Epoch Time: 0 m 53 s\n","\n","Epoch: 7\n","390 391 Loss: 0.537 | Acc: 81.474% (40737/50000)\n","99 100 Loss: 0.692 | Acc: 76.390% (7639/10000)\n","Learning Rate: 0.098796 | Epoch Time: 0 m 54 s\n","\n","Epoch: 8\n","390 391 Loss: 0.507 | Acc: 82.666% (41333/50000)\n","99 100 Loss: 0.570 | Acc: 80.870% (8087/10000)\n","Learning Rate: 0.098429 | Epoch Time: 0 m 53 s\n","\n","Epoch: 9\n","390 391 Loss: 0.478 | Acc: 83.644% (41822/50000)\n","99 100 Loss: 0.778 | Acc: 74.990% (7499/10000)\n","Learning Rate: 0.098015 | Epoch Time: 0 m 53 s\n","\n","Epoch: 10\n","390 391 Loss: 0.462 | Acc: 84.004% (42002/50000)\n","99 100 Loss: 0.690 | Acc: 76.740% (7674/10000)\n","Learning Rate: 0.097553 | Epoch Time: 0 m 53 s\n","\n","Epoch: 11\n","390 391 Loss: 0.447 | Acc: 84.688% (42344/50000)\n","99 100 Loss: 0.544 | Acc: 81.720% (8172/10000)\n","Learning Rate: 0.097044 | Epoch Time: 0 m 53 s\n","\n","Epoch: 12\n","390 391 Loss: 0.427 | Acc: 85.496% (42748/50000)\n","99 100 Loss: 0.721 | Acc: 77.340% (7734/10000)\n","Learning Rate: 0.096489 | Epoch Time: 0 m 53 s\n","\n","Epoch: 13\n","390 391 Loss: 0.416 | Acc: 85.818% (42909/50000)\n","99 100 Loss: 0.653 | Acc: 79.190% (7919/10000)\n","Learning Rate: 0.095888 | Epoch Time: 0 m 53 s\n","\n","Epoch: 14\n","390 391 Loss: 0.407 | Acc: 85.996% (42998/50000)\n","99 100 Loss: 0.845 | Acc: 74.270% (7427/10000)\n","Learning Rate: 0.095241 | Epoch Time: 0 m 53 s\n","\n","Epoch: 15\n","390 391 Loss: 0.397 | Acc: 86.696% (43348/50000)\n","99 100 Loss: 0.556 | Acc: 80.890% (8089/10000)\n","Learning Rate: 0.094550 | Epoch Time: 0 m 53 s\n","\n","Epoch: 16\n","390 391 Loss: 0.384 | Acc: 86.784% (43392/50000)\n","99 100 Loss: 0.456 | Acc: 84.890% (8489/10000)\n","Learning Rate: 0.093815 | Epoch Time: 0 m 53 s\n","\n","Epoch: 17\n","390 391 Loss: 0.380 | Acc: 86.820% (43410/50000)\n","99 100 Loss: 0.500 | Acc: 83.410% (8341/10000)\n","Learning Rate: 0.093037 | Epoch Time: 0 m 53 s\n","\n","Epoch: 18\n","390 391 Loss: 0.372 | Acc: 87.206% (43603/50000)\n","99 100 Loss: 0.525 | Acc: 82.400% (8240/10000)\n","Learning Rate: 0.092216 | Epoch Time: 0 m 54 s\n","\n","Epoch: 19\n","390 391 Loss: 0.360 | Acc: 87.572% (43786/50000)\n","99 100 Loss: 0.527 | Acc: 82.300% (8230/10000)\n","Learning Rate: 0.091354 | Epoch Time: 0 m 53 s\n","\n","Epoch: 20\n","390 391 Loss: 0.348 | Acc: 88.020% (44010/50000)\n","99 100 Loss: 0.558 | Acc: 81.890% (8189/10000)\n","Learning Rate: 0.090451 | Epoch Time: 0 m 53 s\n","\n","Epoch: 21\n","390 391 Loss: 0.347 | Acc: 87.958% (43979/50000)\n","99 100 Loss: 0.465 | Acc: 84.280% (8428/10000)\n","Learning Rate: 0.089508 | Epoch Time: 0 m 53 s\n","\n","Epoch: 22\n","390 391 Loss: 0.336 | Acc: 88.452% (44226/50000)\n","99 100 Loss: 0.575 | Acc: 81.050% (8105/10000)\n","Learning Rate: 0.088526 | Epoch Time: 0 m 53 s\n","\n","Epoch: 23\n","390 391 Loss: 0.333 | Acc: 88.562% (44281/50000)\n","99 100 Loss: 0.489 | Acc: 83.140% (8314/10000)\n","Learning Rate: 0.087506 | Epoch Time: 0 m 53 s\n","\n","Epoch: 24\n","390 391 Loss: 0.328 | Acc: 88.742% (44371/50000)\n","99 100 Loss: 0.484 | Acc: 83.890% (8389/10000)\n","Learning Rate: 0.086448 | Epoch Time: 0 m 53 s\n","\n","Epoch: 25\n","390 391 Loss: 0.318 | Acc: 89.144% (44572/50000)\n","99 100 Loss: 0.633 | Acc: 79.770% (7977/10000)\n","Learning Rate: 0.085355 | Epoch Time: 0 m 53 s\n","\n","Epoch: 26\n","390 391 Loss: 0.313 | Acc: 89.310% (44655/50000)\n","99 100 Loss: 0.474 | Acc: 84.820% (8482/10000)\n","Learning Rate: 0.084227 | Epoch Time: 0 m 53 s\n","\n","Epoch: 27\n","390 391 Loss: 0.310 | Acc: 89.186% (44593/50000)\n","99 100 Loss: 0.567 | Acc: 82.100% (8210/10000)\n","Learning Rate: 0.083066 | Epoch Time: 0 m 53 s\n","\n","Epoch: 28\n","390 391 Loss: 0.300 | Acc: 89.726% (44863/50000)\n","99 100 Loss: 0.444 | Acc: 85.130% (8513/10000)\n","Learning Rate: 0.081871 | Epoch Time: 0 m 53 s\n","\n","Epoch: 29\n","390 391 Loss: 0.299 | Acc: 89.712% (44856/50000)\n","99 100 Loss: 0.399 | Acc: 86.620% (8662/10000)\n","Learning Rate: 0.080645 | Epoch Time: 0 m 53 s\n","\n","Epoch: 30\n","390 391 Loss: 0.290 | Acc: 89.866% (44933/50000)\n","99 100 Loss: 0.507 | Acc: 83.490% (8349/10000)\n","Learning Rate: 0.079389 | Epoch Time: 0 m 53 s\n","\n","Epoch: 31\n","390 391 Loss: 0.285 | Acc: 90.298% (45149/50000)\n","99 100 Loss: 0.482 | Acc: 84.090% (8409/10000)\n","Learning Rate: 0.078104 | Epoch Time: 0 m 53 s\n","\n","Epoch: 32\n","390 391 Loss: 0.279 | Acc: 90.294% (45147/50000)\n","99 100 Loss: 0.458 | Acc: 84.750% (8475/10000)\n","Learning Rate: 0.076791 | Epoch Time: 0 m 54 s\n","\n","Epoch: 33\n","390 391 Loss: 0.274 | Acc: 90.510% (45255/50000)\n","99 100 Loss: 0.555 | Acc: 83.090% (8309/10000)\n","Learning Rate: 0.075452 | Epoch Time: 0 m 53 s\n","\n","Epoch: 34\n","390 391 Loss: 0.269 | Acc: 90.708% (45354/50000)\n","99 100 Loss: 0.536 | Acc: 82.620% (8262/10000)\n","Learning Rate: 0.074088 | Epoch Time: 0 m 53 s\n","\n","Epoch: 35\n","390 391 Loss: 0.265 | Acc: 91.032% (45516/50000)\n","99 100 Loss: 0.458 | Acc: 85.440% (8544/10000)\n","Learning Rate: 0.072700 | Epoch Time: 0 m 53 s\n","\n","Epoch: 36\n","390 391 Loss: 0.260 | Acc: 91.040% (45520/50000)\n","99 100 Loss: 0.343 | Acc: 88.780% (8878/10000)\n","Learning Rate: 0.071289 | Epoch Time: 0 m 53 s\n","\n","Epoch: 37\n","390 391 Loss: 0.253 | Acc: 91.318% (45659/50000)\n","99 100 Loss: 0.487 | Acc: 85.040% (8504/10000)\n","Learning Rate: 0.069857 | Epoch Time: 0 m 53 s\n","\n","Epoch: 38\n","390 391 Loss: 0.254 | Acc: 91.266% (45633/50000)\n","99 100 Loss: 0.500 | Acc: 83.690% (8369/10000)\n","Learning Rate: 0.068406 | Epoch Time: 0 m 53 s\n","\n","Epoch: 39\n","390 391 Loss: 0.238 | Acc: 91.790% (45895/50000)\n","99 100 Loss: 0.514 | Acc: 84.130% (8413/10000)\n","Learning Rate: 0.066937 | Epoch Time: 0 m 53 s\n","\n","Epoch: 40\n","390 391 Loss: 0.243 | Acc: 91.724% (45862/50000)\n","99 100 Loss: 0.474 | Acc: 84.640% (8464/10000)\n","Learning Rate: 0.065451 | Epoch Time: 0 m 53 s\n","\n","Epoch: 41\n","390 391 Loss: 0.236 | Acc: 91.854% (45927/50000)\n","99 100 Loss: 0.392 | Acc: 87.150% (8715/10000)\n","Learning Rate: 0.063950 | Epoch Time: 0 m 53 s\n","\n","Epoch: 42\n","390 391 Loss: 0.227 | Acc: 92.382% (46191/50000)\n","99 100 Loss: 0.346 | Acc: 88.490% (8849/10000)\n","Learning Rate: 0.062434 | Epoch Time: 0 m 53 s\n","\n","Epoch: 43\n","390 391 Loss: 0.226 | Acc: 92.204% (46102/50000)\n","99 100 Loss: 0.451 | Acc: 85.990% (8599/10000)\n","Learning Rate: 0.060907 | Epoch Time: 0 m 53 s\n","\n","Epoch: 44\n","390 391 Loss: 0.222 | Acc: 92.424% (46212/50000)\n","99 100 Loss: 0.328 | Acc: 89.260% (8926/10000)\n","Learning Rate: 0.059369 | Epoch Time: 0 m 53 s\n","\n","Epoch: 45\n","390 391 Loss: 0.219 | Acc: 92.492% (46246/50000)\n","99 100 Loss: 0.416 | Acc: 86.800% (8680/10000)\n","Learning Rate: 0.057822 | Epoch Time: 0 m 54 s\n","\n","Epoch: 46\n","390 391 Loss: 0.210 | Acc: 92.770% (46385/50000)\n","99 100 Loss: 0.368 | Acc: 88.930% (8893/10000)\n","Learning Rate: 0.056267 | Epoch Time: 0 m 53 s\n","\n","Epoch: 47\n","390 391 Loss: 0.206 | Acc: 92.964% (46482/50000)\n","99 100 Loss: 0.339 | Acc: 89.170% (8917/10000)\n","Learning Rate: 0.054705 | Epoch Time: 0 m 53 s\n","\n","Epoch: 48\n","390 391 Loss: 0.197 | Acc: 93.212% (46606/50000)\n","99 100 Loss: 0.365 | Acc: 88.780% (8878/10000)\n","Learning Rate: 0.053140 | Epoch Time: 0 m 54 s\n","\n","Epoch: 49\n","390 391 Loss: 0.195 | Acc: 93.212% (46606/50000)\n","99 100 Loss: 0.381 | Acc: 87.650% (8765/10000)\n","Learning Rate: 0.051571 | Epoch Time: 0 m 53 s\n","\n","Epoch: 50\n","390 391 Loss: 0.188 | Acc: 93.390% (46695/50000)\n","99 100 Loss: 0.296 | Acc: 90.220% (9022/10000)\n","Learning Rate: 0.050000 | Epoch Time: 0 m 53 s\n","\n","Epoch: 51\n","390 391 Loss: 0.178 | Acc: 93.906% (46953/50000)\n","99 100 Loss: 0.341 | Acc: 89.310% (8931/10000)\n","Learning Rate: 0.048429 | Epoch Time: 0 m 53 s\n","\n","Epoch: 52\n","390 391 Loss: 0.175 | Acc: 94.030% (47015/50000)\n","99 100 Loss: 0.312 | Acc: 90.010% (9001/10000)\n","Learning Rate: 0.046860 | Epoch Time: 0 m 53 s\n","\n","Epoch: 53\n","390 391 Loss: 0.168 | Acc: 94.208% (47104/50000)\n","99 100 Loss: 0.355 | Acc: 88.620% (8862/10000)\n","Learning Rate: 0.045295 | Epoch Time: 0 m 53 s\n","\n","Epoch: 54\n","390 391 Loss: 0.161 | Acc: 94.432% (47216/50000)\n","99 100 Loss: 0.313 | Acc: 89.980% (8998/10000)\n","Learning Rate: 0.043733 | Epoch Time: 0 m 53 s\n","\n","Epoch: 55\n","390 391 Loss: 0.156 | Acc: 94.654% (47327/50000)\n","99 100 Loss: 0.343 | Acc: 89.230% (8923/10000)\n","Learning Rate: 0.042178 | Epoch Time: 0 m 53 s\n","\n","Epoch: 56\n","390 391 Loss: 0.154 | Acc: 94.748% (47374/50000)\n","99 100 Loss: 0.500 | Acc: 85.010% (8501/10000)\n","Learning Rate: 0.040631 | Epoch Time: 0 m 53 s\n","\n","Epoch: 57\n","390 391 Loss: 0.149 | Acc: 94.970% (47485/50000)\n","99 100 Loss: 0.333 | Acc: 89.360% (8936/10000)\n","Learning Rate: 0.039093 | Epoch Time: 0 m 53 s\n","\n","Epoch: 58\n","390 391 Loss: 0.138 | Acc: 95.168% (47584/50000)\n","99 100 Loss: 0.347 | Acc: 89.250% (8925/10000)\n","Learning Rate: 0.037566 | Epoch Time: 0 m 53 s\n","\n","Epoch: 59\n","390 391 Loss: 0.134 | Acc: 95.440% (47720/50000)\n","99 100 Loss: 0.345 | Acc: 89.470% (8947/10000)\n","Learning Rate: 0.036050 | Epoch Time: 0 m 53 s\n","\n","Epoch: 60\n","390 391 Loss: 0.128 | Acc: 95.606% (47803/50000)\n","99 100 Loss: 0.302 | Acc: 90.290% (9029/10000)\n","Learning Rate: 0.034549 | Epoch Time: 0 m 53 s\n","\n","Epoch: 61\n","390 391 Loss: 0.122 | Acc: 95.858% (47929/50000)\n","99 100 Loss: 0.322 | Acc: 89.710% (8971/10000)\n","Learning Rate: 0.033063 | Epoch Time: 0 m 53 s\n","\n","Epoch: 62\n","390 391 Loss: 0.114 | Acc: 96.110% (48055/50000)\n","99 100 Loss: 0.314 | Acc: 90.260% (9026/10000)\n","Learning Rate: 0.031594 | Epoch Time: 0 m 53 s\n","\n","Epoch: 63\n","390 391 Loss: 0.107 | Acc: 96.370% (48185/50000)\n","99 100 Loss: 0.325 | Acc: 90.180% (9018/10000)\n","Learning Rate: 0.030143 | Epoch Time: 0 m 53 s\n","\n","Epoch: 64\n","390 391 Loss: 0.101 | Acc: 96.554% (48277/50000)\n","99 100 Loss: 0.309 | Acc: 90.780% (9078/10000)\n","Learning Rate: 0.028711 | Epoch Time: 0 m 53 s\n","\n","Epoch: 65\n","390 391 Loss: 0.097 | Acc: 96.774% (48387/50000)\n","99 100 Loss: 0.353 | Acc: 89.270% (8927/10000)\n","Learning Rate: 0.027300 | Epoch Time: 0 m 53 s\n","\n","Epoch: 66\n","390 391 Loss: 0.091 | Acc: 96.926% (48463/50000)\n","99 100 Loss: 0.274 | Acc: 91.590% (9159/10000)\n","Learning Rate: 0.025912 | Epoch Time: 0 m 53 s\n","\n","Epoch: 67\n","390 391 Loss: 0.085 | Acc: 97.040% (48520/50000)\n","99 100 Loss: 0.303 | Acc: 91.110% (9111/10000)\n","Learning Rate: 0.024548 | Epoch Time: 0 m 53 s\n","\n","Epoch: 68\n","390 391 Loss: 0.070 | Acc: 97.692% (48846/50000)\n","99 100 Loss: 0.292 | Acc: 91.250% (9125/10000)\n","Learning Rate: 0.023209 | Epoch Time: 0 m 53 s\n","\n","Epoch: 69\n","390 391 Loss: 0.071 | Acc: 97.598% (48799/50000)\n","99 100 Loss: 0.270 | Acc: 92.100% (9210/10000)\n","Learning Rate: 0.021896 | Epoch Time: 0 m 53 s\n","\n","Epoch: 70\n","390 391 Loss: 0.064 | Acc: 97.838% (48919/50000)\n","99 100 Loss: 0.315 | Acc: 91.230% (9123/10000)\n","Learning Rate: 0.020611 | Epoch Time: 0 m 53 s\n","\n","Epoch: 71\n","390 391 Loss: 0.057 | Acc: 98.176% (49088/50000)\n","99 100 Loss: 0.261 | Acc: 92.360% (9236/10000)\n","Learning Rate: 0.019355 | Epoch Time: 0 m 53 s\n","\n","Epoch: 72\n","390 391 Loss: 0.050 | Acc: 98.386% (49193/50000)\n","99 100 Loss: 0.260 | Acc: 92.270% (9227/10000)\n","Learning Rate: 0.018129 | Epoch Time: 0 m 53 s\n","\n","Epoch: 73\n","390 391 Loss: 0.041 | Acc: 98.754% (49377/50000)\n","99 100 Loss: 0.259 | Acc: 92.640% (9264/10000)\n","Learning Rate: 0.016934 | Epoch Time: 0 m 53 s\n","\n","Epoch: 74\n","390 391 Loss: 0.039 | Acc: 98.742% (49371/50000)\n","99 100 Loss: 0.248 | Acc: 93.420% (9342/10000)\n","Saving..\n","Learning Rate: 0.015773 | Epoch Time: 0 m 53 s\n","\n","Epoch: 75\n","390 391 Loss: 0.031 | Acc: 99.048% (49524/50000)\n","99 100 Loss: 0.243 | Acc: 92.870% (9287/10000)\n","Learning Rate: 0.014645 | Epoch Time: 0 m 53 s\n","\n","Epoch: 76\n","390 391 Loss: 0.029 | Acc: 99.086% (49543/50000)\n","99 100 Loss: 0.239 | Acc: 93.610% (9361/10000)\n","Saving..\n","Learning Rate: 0.013552 | Epoch Time: 0 m 53 s\n","\n","Epoch: 77\n","390 391 Loss: 0.020 | Acc: 99.506% (49753/50000)\n","99 100 Loss: 0.247 | Acc: 93.700% (9370/10000)\n","Saving..\n","Learning Rate: 0.012494 | Epoch Time: 0 m 53 s\n","\n","Epoch: 78\n","390 391 Loss: 0.018 | Acc: 99.486% (49743/50000)\n","99 100 Loss: 0.231 | Acc: 93.240% (9324/10000)\n","Learning Rate: 0.011474 | Epoch Time: 0 m 53 s\n","\n","Epoch: 79\n","390 391 Loss: 0.015 | Acc: 99.596% (49798/50000)\n","99 100 Loss: 0.207 | Acc: 94.320% (9432/10000)\n","Saving..\n","Learning Rate: 0.010492 | Epoch Time: 0 m 53 s\n","\n","Epoch: 80\n","390 391 Loss: 0.013 | Acc: 99.694% (49847/50000)\n","99 100 Loss: 0.240 | Acc: 93.640% (9364/10000)\n","Learning Rate: 0.009549 | Epoch Time: 0 m 53 s\n","\n","Epoch: 81\n","390 391 Loss: 0.009 | Acc: 99.822% (49911/50000)\n","99 100 Loss: 0.201 | Acc: 94.650% (9465/10000)\n","Saving..\n","Learning Rate: 0.008646 | Epoch Time: 0 m 53 s\n","\n","Epoch: 82\n","390 391 Loss: 0.009 | Acc: 99.808% (49904/50000)\n","99 100 Loss: 0.213 | Acc: 94.360% (9436/10000)\n","Learning Rate: 0.007784 | Epoch Time: 0 m 53 s\n","\n","Epoch: 83\n","390 391 Loss: 0.006 | Acc: 99.916% (49958/50000)\n","99 100 Loss: 0.201 | Acc: 95.010% (9501/10000)\n","Saving..\n","Learning Rate: 0.006963 | Epoch Time: 0 m 53 s\n","\n","Epoch: 84\n","390 391 Loss: 0.005 | Acc: 99.930% (49965/50000)\n","99 100 Loss: 0.197 | Acc: 94.900% (9490/10000)\n","Learning Rate: 0.006185 | Epoch Time: 0 m 53 s\n","\n","Epoch: 85\n","390 391 Loss: 0.004 | Acc: 99.948% (49974/50000)\n","99 100 Loss: 0.190 | Acc: 94.960% (9496/10000)\n","Learning Rate: 0.005450 | Epoch Time: 0 m 53 s\n","\n","Epoch: 86\n","390 391 Loss: 0.003 | Acc: 99.980% (49990/50000)\n","99 100 Loss: 0.186 | Acc: 95.160% (9516/10000)\n","Saving..\n","Learning Rate: 0.004759 | Epoch Time: 0 m 53 s\n","\n","Epoch: 87\n","390 391 Loss: 0.003 | Acc: 99.972% (49986/50000)\n","99 100 Loss: 0.188 | Acc: 95.230% (9523/10000)\n","Saving..\n","Learning Rate: 0.004112 | Epoch Time: 0 m 53 s\n","\n","Epoch: 88\n","390 391 Loss: 0.003 | Acc: 99.982% (49991/50000)\n","99 100 Loss: 0.186 | Acc: 95.250% (9525/10000)\n","Saving..\n","Learning Rate: 0.003511 | Epoch Time: 0 m 53 s\n","\n","Epoch: 89\n","390 391 Loss: 0.003 | Acc: 99.972% (49986/50000)\n","99 100 Loss: 0.182 | Acc: 95.190% (9519/10000)\n","Learning Rate: 0.002956 | Epoch Time: 0 m 53 s\n","\n","Epoch: 90\n","390 391 Loss: 0.003 | Acc: 99.994% (49997/50000)\n","99 100 Loss: 0.181 | Acc: 95.350% (9535/10000)\n","Saving..\n","Learning Rate: 0.002447 | Epoch Time: 0 m 53 s\n","\n","Epoch: 91\n","390 391 Loss: 0.003 | Acc: 99.990% (49995/50000)\n","99 100 Loss: 0.185 | Acc: 95.320% (9532/10000)\n","Learning Rate: 0.001985 | Epoch Time: 0 m 53 s\n","\n","Epoch: 92\n","390 391 Loss: 0.003 | Acc: 99.980% (49990/50000)\n","99 100 Loss: 0.182 | Acc: 95.310% (9531/10000)\n","Learning Rate: 0.001571 | Epoch Time: 0 m 53 s\n","\n","Epoch: 93\n","390 391 Loss: 0.003 | Acc: 99.982% (49991/50000)\n","99 100 Loss: 0.182 | Acc: 95.260% (9526/10000)\n","Learning Rate: 0.001204 | Epoch Time: 0 m 53 s\n","\n","Epoch: 94\n","390 391 Loss: 0.003 | Acc: 99.990% (49995/50000)\n","99 100 Loss: 0.182 | Acc: 95.420% (9542/10000)\n","Saving..\n","Learning Rate: 0.000886 | Epoch Time: 0 m 53 s\n","\n","Epoch: 95\n","390 391 Loss: 0.002 | Acc: 99.994% (49997/50000)\n","99 100 Loss: 0.181 | Acc: 95.360% (9536/10000)\n","Learning Rate: 0.000616 | Epoch Time: 0 m 53 s\n","\n","Epoch: 96\n","390 391 Loss: 0.003 | Acc: 99.992% (49996/50000)\n","99 100 Loss: 0.180 | Acc: 95.320% (9532/10000)\n","Learning Rate: 0.000394 | Epoch Time: 0 m 53 s\n","\n","Epoch: 97\n","390 391 Loss: 0.002 | Acc: 99.994% (49997/50000)\n","99 100 Loss: 0.180 | Acc: 95.430% (9543/10000)\n","Saving..\n","Learning Rate: 0.000222 | Epoch Time: 0 m 53 s\n","\n","Epoch: 98\n","390 391 Loss: 0.003 | Acc: 99.986% (49993/50000)\n","99 100 Loss: 0.181 | Acc: 95.370% (9537/10000)\n","Learning Rate: 0.000099 | Epoch Time: 0 m 53 s\n","\n","Epoch: 99\n","390 391 Loss: 0.002 | Acc: 99.990% (49995/50000)\n","99 100 Loss: 0.182 | Acc: 95.330% (9533/10000)\n","Learning Rate: 0.000025 | Epoch Time: 0 m 53 s\n"]}],"source":["# TODO: Change the model to your own selection\n","name = \"model_1_2\"\n","name = name.lower()\n","resume = None\n","# resume = f\"./checkpoint/{name}/001.pth\" # change the checkpoint name to the one desired\n","\n","# Model\n","print('==> Building model..')\n","net = ResNet_custom(name).to(device)\n","\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","if resume is not None:\n","    # Load checkpoint.\n","    print(f'==> Resuming from {resume}..')\n","    assert os.path.exists(resume), 'Error: no checkpoint found!'\n","    checkpoint = torch.load(resume)\n","    assert name == checkpoint['name'], 'Error: model does not match checkpoint!'\n","    net.load_state_dict(checkpoint['net'])\n","    best_acc = checkpoint['acc']\n","    start_epoch = checkpoint['epoch']\n","\n","# TODO: Loss\n","criterion = nn.CrossEntropyLoss()\n","# TODO: optimizer, SGD vs Adam, learning rate selection, etc...\n","optimizer = optim.SGD(net.parameters(), lr=0.1,\n","                      momentum=0.9, weight_decay=5e-4)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n","\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","    train_loss_list.append(train_loss/(batch_idx+1))\n","    train_ACC_list.append(100.*correct/total)\n","\n","\n","    print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","        test_loss_list.append(test_loss/(batch_idx+1))\n","        test_ACC_list.append(100.*correct/total)\n","\n","        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'name': name,\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir(f'checkpoint/{name}'):\n","            os.mkdir(f'checkpoint/{name}')\n","        torch.save(state, f'./checkpoint/{name}/{epoch:03}.pth')\n","        best_acc = acc\n","\n","train_loss_list = []\n","test_loss_list = []\n","train_ACC_list = []\n","test_ACC_list = []\n","lr_list = []\n","\n","# TODO: decide when the training should stop\n","for epoch in range(start_epoch, start_epoch+100):\n","    start_time = time.time()\n","\n","    train(epoch)\n","    test(epoch)\n","\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(\"Learning Rate: %f | Epoch Time: %i m %i s\" % (optimizer.param_groups[0]['lr'], epoch_mins,epoch_secs))\n","\n","    lr_list.append(optimizer.param_groups[0]['lr'])\n","\n","    scheduler.step()\n","\n","    # break # for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7s0ELIRorDbm"},"outputs":[],"source":["result = pd.DataFrame(np.array([train_loss_list, test_loss_list,\n","                       train_ACC_list, test_ACC_list,\n","                       lr_list]).T, columns=['train_loss','test_loss','train_ACC','test_ACC','lr'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKOHCEzGrFvO"},"outputs":[],"source":["if not os.path.isdir(f'checkpoint/{name}'):\n","    os.mkdir(f'checkpoint/{name}')\n","result.to_csv(f'./checkpoint/{name}/result.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JClBuaqrgF5J"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"sYv75PpXgGb5"},"source":["# Model_1_3 (Adam)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"executionInfo":{"elapsed":59246,"status":"error","timestamp":1668738135275,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"mwtllxfKgGb6","outputId":"19d5d9d3-fb68-4e57-bb39-7dcb97245882"},"outputs":[{"name":"stdout","output_type":"stream","text":["==> Building model..\n","\n","Epoch: 0\n","390 391 Loss: 1.729 | Acc: 34.964% (17482/50000)\n","99 100 Loss: 1.420 | Acc: 48.670% (4867/10000)\n","Learning Rate: 0.010000 | Epoch Time: 0 m 55 s\n","\n","Epoch: 1\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-da63b9975684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-91-da63b9975684>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# TODO: Change the model to your own selection\n","name = \"model_1_3\"\n","name = name.lower()\n","resume = None\n","# resume = f\"./checkpoint/{name}/001.pth\" # change the checkpoint name to the one desired\n","\n","# Model\n","print('==> Building model..')\n","net = ResNet_custom(name).to(device)\n","\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","if resume is not None:\n","    # Load checkpoint.\n","    print(f'==> Resuming from {resume}..')\n","    assert os.path.exists(resume), 'Error: no checkpoint found!'\n","    checkpoint = torch.load(resume)\n","    assert name == checkpoint['name'], 'Error: model does not match checkpoint!'\n","    net.load_state_dict(checkpoint['net'])\n","    best_acc = checkpoint['acc']\n","    start_epoch = checkpoint['epoch']\n","\n","# TODO: Loss\n","criterion = nn.CrossEntropyLoss()\n","# TODO: optimizer, SGD vs Adam, learning rate selection, etc...\n","optimizer = optim.Adam(net.parameters(), lr=0.01)\n","#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n","\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","    train_loss_list.append(train_loss/(batch_idx+1))\n","    train_ACC_list.append(100.*correct/total)\n","\n","\n","    print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","        test_loss_list.append(test_loss/(batch_idx+1))\n","        test_ACC_list.append(100.*correct/total)\n","\n","        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'name': name,\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir(f'checkpoint/{name}'):\n","            os.mkdir(f'checkpoint/{name}')\n","        torch.save(state, f'./checkpoint/{name}/{epoch:03}.pth')\n","        best_acc = acc\n","\n","train_loss_list = []\n","test_loss_list = []\n","train_ACC_list = []\n","test_ACC_list = []\n","lr_list = []\n","\n","# TODO: decide when the training should stop\n","for epoch in range(start_epoch, start_epoch+100):\n","    start_time = time.time()\n","\n","    train(epoch)\n","    test(epoch)\n","\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(\"Learning Rate: %f | Epoch Time: %i m %i s\" % (optimizer.param_groups[0]['lr'], epoch_mins,epoch_secs))\n","\n","    lr_list.append(optimizer.param_groups[0]['lr'])\n","\n","    #scheduler.step()\n","\n","    # break # for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ut-RKzlgGb7"},"outputs":[],"source":["result = pd.DataFrame(np.array([train_loss_list, test_loss_list,\n","                       train_ACC_list, test_ACC_list,\n","                       lr_list]).T, columns=['train_loss','test_loss','train_ACC','test_ACC','lr'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cry1BUgwgGb7"},"outputs":[],"source":["if not os.path.isdir(f'checkpoint/{name}'):\n","    os.mkdir(f'checkpoint/{name}')\n","result.to_csv(f'./checkpoint/{name}/result.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j5u4WThIi22Q"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Q_LX_RGOi3Kz"},"source":["# Model_1_4 (Adam Decay lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"elapsed":118120,"status":"error","timestamp":1668738306056,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"qfaFernxi3K0","outputId":"57d849c6-0027-4022-c5be-586a2c7248fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["==> Building model..\n","\n","Epoch: 0\n","390 391 Loss: 1.832 | Acc: 31.546% (15773/50000)\n","99 100 Loss: 1.521 | Acc: 42.800% (4280/10000)\n","Learning Rate: 0.010000 | Epoch Time: 0 m 54 s\n","\n","Epoch: 1\n","390 391 Loss: 1.373 | Acc: 49.628% (24814/50000)\n","99 100 Loss: 1.200 | Acc: 57.000% (5700/10000)\n","Learning Rate: 0.009998 | Epoch Time: 0 m 54 s\n","\n","Epoch: 2\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-92-56b0085b610e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-92-56b0085b610e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# TODO: Change the model to your own selection\n","name = \"model_1_4\"\n","name = name.lower()\n","resume = None\n","# resume = f\"./checkpoint/{name}/001.pth\" # change the checkpoint name to the one desired\n","\n","# Model\n","print('==> Building model..')\n","net = ResNet_custom(name).to(device)\n","\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","if resume is not None:\n","    # Load checkpoint.\n","    print(f'==> Resuming from {resume}..')\n","    assert os.path.exists(resume), 'Error: no checkpoint found!'\n","    checkpoint = torch.load(resume)\n","    assert name == checkpoint['name'], 'Error: model does not match checkpoint!'\n","    net.load_state_dict(checkpoint['net'])\n","    best_acc = checkpoint['acc']\n","    start_epoch = checkpoint['epoch']\n","\n","# TODO: Loss\n","criterion = nn.CrossEntropyLoss()\n","# TODO: optimizer, SGD vs Adam, learning rate selection, etc...\n","optimizer = optim.Adam(net.parameters(), lr=0.01)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n","\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","    train_loss_list.append(train_loss/(batch_idx+1))\n","    train_ACC_list.append(100.*correct/total)\n","\n","\n","    print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","        test_loss_list.append(test_loss/(batch_idx+1))\n","        test_ACC_list.append(100.*correct/total)\n","\n","        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'name': name,\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir(f'checkpoint/{name}'):\n","            os.mkdir(f'checkpoint/{name}')\n","        torch.save(state, f'./checkpoint/{name}/{epoch:03}.pth')\n","        best_acc = acc\n","\n","train_loss_list = []\n","test_loss_list = []\n","train_ACC_list = []\n","test_ACC_list = []\n","lr_list = []\n","\n","# TODO: decide when the training should stop\n","for epoch in range(start_epoch, start_epoch+100):\n","    start_time = time.time()\n","\n","    train(epoch)\n","    test(epoch)\n","\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(\"Learning Rate: %f | Epoch Time: %i m %i s\" % (optimizer.param_groups[0]['lr'], epoch_mins,epoch_secs))\n","\n","    lr_list.append(optimizer.param_groups[0]['lr'])\n","\n","    scheduler.step()\n","\n","    # break # for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_7yeSciMi3K0"},"outputs":[],"source":["result = pd.DataFrame(np.array([train_loss_list, test_loss_list,\n","                       train_ACC_list, test_ACC_list,\n","                       lr_list]).T, columns=['train_loss','test_loss','train_ACC','test_ACC','lr'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvoPzAvqi3K0"},"outputs":[],"source":["if not os.path.isdir(f'checkpoint/{name}'):\n","    os.mkdir(f'checkpoint/{name}')\n","result.to_csv(f'./checkpoint/{name}/result.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7oSDBv1j0eo"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_owV7kPJj5CN"},"source":["# Model_2_1 (with constant lr=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":558,"status":"ok","timestamp":1668738433835,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"NAgntWLHj5CO","outputId":"bd481b02-98fb-4583-8001-f4f72956a94c"},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","ResNet                                   [10, 10]                  --\n","├─Conv2d: 1-1                            [10, 64, 32, 32]          1,728\n","├─BatchNorm2d: 1-2                       [10, 64, 32, 32]          128\n","├─Sequential: 1-3                        [10, 256, 32, 32]         --\n","│    └─Bottleneck: 2-1                   [10, 256, 32, 32]         --\n","│    │    └─Conv2d: 3-1                  [10, 64, 32, 32]          4,096\n","│    │    └─BatchNorm2d: 3-2             [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-3                  [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-4             [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-5                  [10, 256, 32, 32]         16,384\n","│    │    └─BatchNorm2d: 3-6             [10, 256, 32, 32]         512\n","│    │    └─Sequential: 3-7              [10, 256, 32, 32]         16,896\n","│    └─Bottleneck: 2-2                   [10, 256, 32, 32]         --\n","│    │    └─Conv2d: 3-8                  [10, 64, 32, 32]          16,384\n","│    │    └─BatchNorm2d: 3-9             [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-10                 [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-11            [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-12                 [10, 256, 32, 32]         16,384\n","│    │    └─BatchNorm2d: 3-13            [10, 256, 32, 32]         512\n","│    │    └─Sequential: 3-14             [10, 256, 32, 32]         --\n","│    └─Bottleneck: 2-3                   [10, 256, 32, 32]         --\n","│    │    └─Conv2d: 3-15                 [10, 64, 32, 32]          16,384\n","│    │    └─BatchNorm2d: 3-16            [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-17                 [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-18            [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-19                 [10, 256, 32, 32]         16,384\n","│    │    └─BatchNorm2d: 3-20            [10, 256, 32, 32]         512\n","│    │    └─Sequential: 3-21             [10, 256, 32, 32]         --\n","├─Sequential: 1-4                        [10, 512, 16, 16]         --\n","│    └─Bottleneck: 2-4                   [10, 512, 16, 16]         --\n","│    │    └─Conv2d: 3-22                 [10, 128, 32, 32]         32,768\n","│    │    └─BatchNorm2d: 3-23            [10, 128, 32, 32]         256\n","│    │    └─Conv2d: 3-24                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-25            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-26                 [10, 512, 16, 16]         65,536\n","│    │    └─BatchNorm2d: 3-27            [10, 512, 16, 16]         1,024\n","│    │    └─Sequential: 3-28             [10, 512, 16, 16]         132,096\n","│    └─Bottleneck: 2-5                   [10, 512, 16, 16]         --\n","│    │    └─Conv2d: 3-29                 [10, 128, 16, 16]         65,536\n","│    │    └─BatchNorm2d: 3-30            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-31                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-32            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-33                 [10, 512, 16, 16]         65,536\n","│    │    └─BatchNorm2d: 3-34            [10, 512, 16, 16]         1,024\n","│    │    └─Sequential: 3-35             [10, 512, 16, 16]         --\n","│    └─Bottleneck: 2-6                   [10, 512, 16, 16]         --\n","│    │    └─Conv2d: 3-36                 [10, 128, 16, 16]         65,536\n","│    │    └─BatchNorm2d: 3-37            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-38                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-39            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-40                 [10, 512, 16, 16]         65,536\n","│    │    └─BatchNorm2d: 3-41            [10, 512, 16, 16]         1,024\n","│    │    └─Sequential: 3-42             [10, 512, 16, 16]         --\n","├─Sequential: 1-5                        [10, 1024, 8, 8]          --\n","│    └─Bottleneck: 2-7                   [10, 1024, 8, 8]          --\n","│    │    └─Conv2d: 3-43                 [10, 256, 16, 16]         131,072\n","│    │    └─BatchNorm2d: 3-44            [10, 256, 16, 16]         512\n","│    │    └─Conv2d: 3-45                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-46            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-47                 [10, 1024, 8, 8]          262,144\n","│    │    └─BatchNorm2d: 3-48            [10, 1024, 8, 8]          2,048\n","│    │    └─Sequential: 3-49             [10, 1024, 8, 8]          526,336\n","│    └─Bottleneck: 2-8                   [10, 1024, 8, 8]          --\n","│    │    └─Conv2d: 3-50                 [10, 256, 8, 8]           262,144\n","│    │    └─BatchNorm2d: 3-51            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-52                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-53            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-54                 [10, 1024, 8, 8]          262,144\n","│    │    └─BatchNorm2d: 3-55            [10, 1024, 8, 8]          2,048\n","│    │    └─Sequential: 3-56             [10, 1024, 8, 8]          --\n","│    └─Bottleneck: 2-9                   [10, 1024, 8, 8]          --\n","│    │    └─Conv2d: 3-57                 [10, 256, 8, 8]           262,144\n","│    │    └─BatchNorm2d: 3-58            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-59                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-60            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-61                 [10, 1024, 8, 8]          262,144\n","│    │    └─BatchNorm2d: 3-62            [10, 1024, 8, 8]          2,048\n","│    │    └─Sequential: 3-63             [10, 1024, 8, 8]          --\n","├─AdaptiveAvgPool2d: 1-6                 [10, 1024, 1, 1]          --\n","├─Linear: 1-7                            [10, 10]                  10,250\n","==========================================================================================\n","Total params: 4,914,250\n","Trainable params: 4,914,250\n","Non-trainable params: 0\n","Total mult-adds (G): 7.48\n","==========================================================================================\n","Input size (MB): 0.12\n","Forward/backward pass size (MB): 437.78\n","Params size (MB): 19.66\n","Estimated Total Size (MB): 457.56\n","=========================================================================================="]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["summary(ResNet_custom('model_2_1'), (10,3, 32, 32), depth=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10455805,"status":"ok","timestamp":1668748965691,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"x0_cz813j5CO","outputId":"b523bac0-af63-491c-dc59-abcaf71be7ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["==> Building model..\n","\n","Epoch: 0\n","390 391 Loss: 2.174 | Acc: 24.928% (12464/50000)\n","99 100 Loss: 1.674 | Acc: 38.000% (3800/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 46 s\n","\n","Epoch: 1\n","390 391 Loss: 1.563 | Acc: 42.146% (21073/50000)\n","99 100 Loss: 1.411 | Acc: 47.830% (4783/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 2\n","390 391 Loss: 1.334 | Acc: 51.208% (25604/50000)\n","99 100 Loss: 1.378 | Acc: 50.030% (5003/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 3\n","390 391 Loss: 1.135 | Acc: 59.070% (29535/50000)\n","99 100 Loss: 1.162 | Acc: 59.080% (5908/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 4\n","390 391 Loss: 1.007 | Acc: 64.070% (32035/50000)\n","99 100 Loss: 1.276 | Acc: 57.760% (5776/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 5\n","390 391 Loss: 0.905 | Acc: 68.082% (34041/50000)\n","99 100 Loss: 1.089 | Acc: 61.930% (6193/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 6\n","390 391 Loss: 0.828 | Acc: 70.550% (35275/50000)\n","99 100 Loss: 1.018 | Acc: 65.830% (6583/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 7\n","390 391 Loss: 0.775 | Acc: 72.606% (36303/50000)\n","99 100 Loss: 0.889 | Acc: 69.410% (6941/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 8\n","390 391 Loss: 0.724 | Acc: 74.630% (37315/50000)\n","99 100 Loss: 1.061 | Acc: 66.900% (6690/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 9\n","390 391 Loss: 0.685 | Acc: 76.006% (38003/50000)\n","99 100 Loss: 1.004 | Acc: 66.410% (6641/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 10\n","390 391 Loss: 0.655 | Acc: 77.092% (38546/50000)\n","99 100 Loss: 1.294 | Acc: 59.190% (5919/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 11\n","390 391 Loss: 0.627 | Acc: 78.160% (39080/50000)\n","99 100 Loss: 0.768 | Acc: 74.200% (7420/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 12\n","390 391 Loss: 0.603 | Acc: 79.156% (39578/50000)\n","99 100 Loss: 0.895 | Acc: 71.630% (7163/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 13\n","390 391 Loss: 0.593 | Acc: 79.498% (39749/50000)\n","99 100 Loss: 0.683 | Acc: 76.740% (7674/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 14\n","390 391 Loss: 0.572 | Acc: 80.176% (40088/50000)\n","99 100 Loss: 0.694 | Acc: 75.650% (7565/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 15\n","390 391 Loss: 0.551 | Acc: 80.810% (40405/50000)\n","99 100 Loss: 0.958 | Acc: 70.560% (7056/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 16\n","390 391 Loss: 0.537 | Acc: 81.688% (40844/50000)\n","99 100 Loss: 0.827 | Acc: 72.920% (7292/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 17\n","390 391 Loss: 0.527 | Acc: 81.786% (40893/50000)\n","99 100 Loss: 0.760 | Acc: 74.570% (7457/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 18\n","390 391 Loss: 0.509 | Acc: 82.538% (41269/50000)\n","99 100 Loss: 0.747 | Acc: 74.400% (7440/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 19\n","390 391 Loss: 0.500 | Acc: 82.812% (41406/50000)\n","99 100 Loss: 0.848 | Acc: 75.060% (7506/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 20\n","390 391 Loss: 0.494 | Acc: 82.848% (41424/50000)\n","99 100 Loss: 1.110 | Acc: 66.580% (6658/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 21\n","390 391 Loss: 0.478 | Acc: 83.388% (41694/50000)\n","99 100 Loss: 0.869 | Acc: 71.420% (7142/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 22\n","390 391 Loss: 0.475 | Acc: 83.572% (41786/50000)\n","99 100 Loss: 0.805 | Acc: 73.510% (7351/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 23\n","390 391 Loss: 0.463 | Acc: 83.912% (41956/50000)\n","99 100 Loss: 0.869 | Acc: 74.480% (7448/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 24\n","390 391 Loss: 0.458 | Acc: 84.038% (42019/50000)\n","99 100 Loss: 0.674 | Acc: 78.150% (7815/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 25\n","390 391 Loss: 0.451 | Acc: 84.412% (42206/50000)\n","99 100 Loss: 0.595 | Acc: 80.390% (8039/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 26\n","390 391 Loss: 0.447 | Acc: 84.734% (42367/50000)\n","99 100 Loss: 0.734 | Acc: 76.000% (7600/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 27\n","390 391 Loss: 0.433 | Acc: 85.138% (42569/50000)\n","99 100 Loss: 0.599 | Acc: 80.430% (8043/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 28\n","390 391 Loss: 0.435 | Acc: 85.102% (42551/50000)\n","99 100 Loss: 0.624 | Acc: 80.120% (8012/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 29\n","390 391 Loss: 0.436 | Acc: 84.966% (42483/50000)\n","99 100 Loss: 1.108 | Acc: 69.750% (6975/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 30\n","390 391 Loss: 0.428 | Acc: 85.242% (42621/50000)\n","99 100 Loss: 0.658 | Acc: 79.030% (7903/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 31\n","390 391 Loss: 0.430 | Acc: 85.246% (42623/50000)\n","99 100 Loss: 0.685 | Acc: 77.480% (7748/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 32\n","390 391 Loss: 0.424 | Acc: 85.424% (42712/50000)\n","99 100 Loss: 0.789 | Acc: 76.330% (7633/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 33\n","390 391 Loss: 0.422 | Acc: 85.298% (42649/50000)\n","99 100 Loss: 0.613 | Acc: 80.080% (8008/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 34\n","390 391 Loss: 0.415 | Acc: 85.650% (42825/50000)\n","99 100 Loss: 0.655 | Acc: 78.160% (7816/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 35\n","390 391 Loss: 0.415 | Acc: 85.486% (42743/50000)\n","99 100 Loss: 0.508 | Acc: 82.330% (8233/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 36\n","390 391 Loss: 0.415 | Acc: 85.876% (42938/50000)\n","99 100 Loss: 0.753 | Acc: 75.790% (7579/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 37\n","390 391 Loss: 0.407 | Acc: 86.050% (43025/50000)\n","99 100 Loss: 0.742 | Acc: 76.120% (7612/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 38\n","390 391 Loss: 0.408 | Acc: 86.050% (43025/50000)\n","99 100 Loss: 0.658 | Acc: 78.440% (7844/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 39\n","390 391 Loss: 0.406 | Acc: 86.084% (43042/50000)\n","99 100 Loss: 0.903 | Acc: 69.730% (6973/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 40\n","390 391 Loss: 0.401 | Acc: 86.164% (43082/50000)\n","99 100 Loss: 0.748 | Acc: 77.410% (7741/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 41\n","390 391 Loss: 0.398 | Acc: 86.444% (43222/50000)\n","99 100 Loss: 0.753 | Acc: 75.030% (7503/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 42\n","390 391 Loss: 0.391 | Acc: 86.428% (43214/50000)\n","99 100 Loss: 0.752 | Acc: 76.240% (7624/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 43\n","390 391 Loss: 0.396 | Acc: 86.396% (43198/50000)\n","99 100 Loss: 0.656 | Acc: 78.260% (7826/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 44\n","390 391 Loss: 0.396 | Acc: 86.326% (43163/50000)\n","99 100 Loss: 0.968 | Acc: 71.450% (7145/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 45\n","390 391 Loss: 0.392 | Acc: 86.558% (43279/50000)\n","99 100 Loss: 0.704 | Acc: 78.220% (7822/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 46\n","390 391 Loss: 0.389 | Acc: 86.540% (43270/50000)\n","99 100 Loss: 0.862 | Acc: 73.260% (7326/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 47\n","390 391 Loss: 0.389 | Acc: 86.594% (43297/50000)\n","99 100 Loss: 0.530 | Acc: 82.700% (8270/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 48\n","390 391 Loss: 0.384 | Acc: 86.830% (43415/50000)\n","99 100 Loss: 0.663 | Acc: 78.960% (7896/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 49\n","390 391 Loss: 0.384 | Acc: 86.792% (43396/50000)\n","99 100 Loss: 0.797 | Acc: 76.210% (7621/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 50\n","390 391 Loss: 0.383 | Acc: 86.868% (43434/50000)\n","99 100 Loss: 0.753 | Acc: 75.490% (7549/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 51\n","390 391 Loss: 0.384 | Acc: 86.772% (43386/50000)\n","99 100 Loss: 0.572 | Acc: 81.230% (8123/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 52\n","390 391 Loss: 0.384 | Acc: 86.980% (43490/50000)\n","99 100 Loss: 0.629 | Acc: 80.190% (8019/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 53\n","390 391 Loss: 0.382 | Acc: 86.928% (43464/50000)\n","99 100 Loss: 0.742 | Acc: 78.270% (7827/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 54\n","390 391 Loss: 0.378 | Acc: 87.024% (43512/50000)\n","99 100 Loss: 0.648 | Acc: 79.580% (7958/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 55\n","390 391 Loss: 0.383 | Acc: 86.886% (43443/50000)\n","99 100 Loss: 0.531 | Acc: 82.400% (8240/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 56\n","390 391 Loss: 0.372 | Acc: 87.242% (43621/50000)\n","99 100 Loss: 0.680 | Acc: 78.550% (7855/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 57\n","390 391 Loss: 0.374 | Acc: 87.150% (43575/50000)\n","99 100 Loss: 0.556 | Acc: 81.440% (8144/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 58\n","390 391 Loss: 0.378 | Acc: 86.990% (43495/50000)\n","99 100 Loss: 0.617 | Acc: 79.680% (7968/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 59\n","390 391 Loss: 0.373 | Acc: 87.098% (43549/50000)\n","99 100 Loss: 0.506 | Acc: 82.900% (8290/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 60\n","390 391 Loss: 0.369 | Acc: 87.060% (43530/50000)\n","99 100 Loss: 0.983 | Acc: 71.240% (7124/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 61\n","390 391 Loss: 0.371 | Acc: 87.190% (43595/50000)\n","99 100 Loss: 0.574 | Acc: 81.230% (8123/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 62\n","390 391 Loss: 0.365 | Acc: 87.354% (43677/50000)\n","99 100 Loss: 0.523 | Acc: 82.700% (8270/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 63\n","390 391 Loss: 0.369 | Acc: 87.294% (43647/50000)\n","99 100 Loss: 0.514 | Acc: 82.160% (8216/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 64\n","390 391 Loss: 0.370 | Acc: 87.214% (43607/50000)\n","99 100 Loss: 0.668 | Acc: 78.710% (7871/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 65\n","390 391 Loss: 0.372 | Acc: 87.266% (43633/50000)\n","99 100 Loss: 0.654 | Acc: 79.370% (7937/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 66\n","390 391 Loss: 0.367 | Acc: 87.368% (43684/50000)\n","99 100 Loss: 0.613 | Acc: 79.510% (7951/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 67\n","390 391 Loss: 0.362 | Acc: 87.600% (43800/50000)\n","99 100 Loss: 0.473 | Acc: 84.130% (8413/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 68\n","390 391 Loss: 0.365 | Acc: 87.364% (43682/50000)\n","99 100 Loss: 0.515 | Acc: 82.810% (8281/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 69\n","390 391 Loss: 0.365 | Acc: 87.412% (43706/50000)\n","99 100 Loss: 0.567 | Acc: 82.540% (8254/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 70\n","390 391 Loss: 0.365 | Acc: 87.394% (43697/50000)\n","99 100 Loss: 0.492 | Acc: 83.480% (8348/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 71\n","390 391 Loss: 0.361 | Acc: 87.498% (43749/50000)\n","99 100 Loss: 0.595 | Acc: 80.290% (8029/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 72\n","390 391 Loss: 0.366 | Acc: 87.318% (43659/50000)\n","99 100 Loss: 0.909 | Acc: 74.060% (7406/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 73\n","390 391 Loss: 0.359 | Acc: 87.576% (43788/50000)\n","99 100 Loss: 0.664 | Acc: 78.380% (7838/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 74\n","390 391 Loss: 0.364 | Acc: 87.542% (43771/50000)\n","99 100 Loss: 0.545 | Acc: 81.730% (8173/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 75\n","390 391 Loss: 0.359 | Acc: 87.758% (43879/50000)\n","99 100 Loss: 0.533 | Acc: 81.890% (8189/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 76\n","390 391 Loss: 0.360 | Acc: 87.552% (43776/50000)\n","99 100 Loss: 0.514 | Acc: 82.850% (8285/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 77\n","390 391 Loss: 0.353 | Acc: 87.868% (43934/50000)\n","99 100 Loss: 0.641 | Acc: 80.150% (8015/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 78\n","390 391 Loss: 0.359 | Acc: 87.726% (43863/50000)\n","99 100 Loss: 0.634 | Acc: 79.240% (7924/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 79\n","390 391 Loss: 0.354 | Acc: 87.838% (43919/50000)\n","99 100 Loss: 0.696 | Acc: 77.230% (7723/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 80\n","390 391 Loss: 0.359 | Acc: 87.398% (43699/50000)\n","99 100 Loss: 0.490 | Acc: 83.950% (8395/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 81\n","390 391 Loss: 0.357 | Acc: 87.682% (43841/50000)\n","99 100 Loss: 0.486 | Acc: 83.830% (8383/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 82\n","390 391 Loss: 0.361 | Acc: 87.668% (43834/50000)\n","99 100 Loss: 0.555 | Acc: 82.300% (8230/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 83\n","390 391 Loss: 0.351 | Acc: 87.814% (43907/50000)\n","99 100 Loss: 0.557 | Acc: 80.810% (8081/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 84\n","390 391 Loss: 0.355 | Acc: 87.788% (43894/50000)\n","99 100 Loss: 0.700 | Acc: 78.870% (7887/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 85\n","390 391 Loss: 0.353 | Acc: 87.848% (43924/50000)\n","99 100 Loss: 0.540 | Acc: 82.000% (8200/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 86\n","390 391 Loss: 0.353 | Acc: 87.698% (43849/50000)\n","99 100 Loss: 0.497 | Acc: 83.520% (8352/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 87\n","390 391 Loss: 0.352 | Acc: 87.980% (43990/50000)\n","99 100 Loss: 0.882 | Acc: 72.610% (7261/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 88\n","390 391 Loss: 0.354 | Acc: 87.908% (43954/50000)\n","99 100 Loss: 0.507 | Acc: 83.920% (8392/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 89\n","390 391 Loss: 0.349 | Acc: 87.970% (43985/50000)\n","99 100 Loss: 0.547 | Acc: 81.930% (8193/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 90\n","390 391 Loss: 0.356 | Acc: 87.692% (43846/50000)\n","99 100 Loss: 0.522 | Acc: 82.930% (8293/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 91\n","390 391 Loss: 0.351 | Acc: 88.020% (44010/50000)\n","99 100 Loss: 0.593 | Acc: 80.730% (8073/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 92\n","390 391 Loss: 0.353 | Acc: 87.980% (43990/50000)\n","99 100 Loss: 0.512 | Acc: 82.770% (8277/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 93\n","390 391 Loss: 0.350 | Acc: 87.918% (43959/50000)\n","99 100 Loss: 0.449 | Acc: 84.900% (8490/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 94\n","390 391 Loss: 0.350 | Acc: 87.902% (43951/50000)\n","99 100 Loss: 0.697 | Acc: 77.750% (7775/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 95\n","390 391 Loss: 0.355 | Acc: 87.610% (43805/50000)\n","99 100 Loss: 0.539 | Acc: 81.760% (8176/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 96\n","390 391 Loss: 0.352 | Acc: 87.906% (43953/50000)\n","99 100 Loss: 0.755 | Acc: 76.250% (7625/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 97\n","390 391 Loss: 0.348 | Acc: 88.128% (44064/50000)\n","99 100 Loss: 0.595 | Acc: 81.130% (8113/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 98\n","390 391 Loss: 0.348 | Acc: 87.920% (43960/50000)\n","99 100 Loss: 0.536 | Acc: 82.220% (8222/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 99\n","390 391 Loss: 0.347 | Acc: 88.198% (44099/50000)\n","99 100 Loss: 0.532 | Acc: 82.050% (8205/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n"]}],"source":["# TODO: Change the model to your own selection\n","name = \"model_2_1\"\n","name = name.lower()\n","resume = None\n","# resume = f\"./checkpoint/{name}/001.pth\" # change the checkpoint name to the one desired\n","\n","# Model\n","print('==> Building model..')\n","net = ResNet_custom(name).to(device)\n","\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","if resume is not None:\n","    # Load checkpoint.\n","    print(f'==> Resuming from {resume}..')\n","    assert os.path.exists(resume), 'Error: no checkpoint found!'\n","    checkpoint = torch.load(resume)\n","    assert name == checkpoint['name'], 'Error: model does not match checkpoint!'\n","    net.load_state_dict(checkpoint['net'])\n","    best_acc = checkpoint['acc']\n","    start_epoch = checkpoint['epoch']\n","\n","# TODO: Loss\n","criterion = nn.CrossEntropyLoss()\n","# TODO: optimizer, SGD vs Adam, learning rate selection, etc...\n","optimizer = optim.SGD(net.parameters(), lr=0.1,\n","                      momentum=0.9, weight_decay=5e-4)\n","#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n","\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","    train_loss_list.append(train_loss/(batch_idx+1))\n","    train_ACC_list.append(100.*correct/total)\n","\n","\n","    print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","        test_loss_list.append(test_loss/(batch_idx+1))\n","        test_ACC_list.append(100.*correct/total)\n","\n","        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'name': name,\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir(f'checkpoint/{name}'):\n","            os.mkdir(f'checkpoint/{name}')\n","        torch.save(state, f'./checkpoint/{name}/{epoch:03}.pth')\n","        best_acc = acc\n","\n","train_loss_list = []\n","test_loss_list = []\n","train_ACC_list = []\n","test_ACC_list = []\n","lr_list = []\n","\n","# TODO: decide when the training should stop\n","for epoch in range(start_epoch, start_epoch+100):\n","    start_time = time.time()\n","\n","    train(epoch)\n","    test(epoch)\n","\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(\"Learning Rate: %f | Epoch Time: %i m %i s\" % (optimizer.param_groups[0]['lr'], epoch_mins,epoch_secs))\n","\n","    lr_list.append(optimizer.param_groups[0]['lr'])\n","\n","    #scheduler.step()\n","\n","    # break # for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWqLljYXj5CO"},"outputs":[],"source":["result = pd.DataFrame(np.array([train_loss_list, test_loss_list,\n","                       train_ACC_list, test_ACC_list,\n","                       lr_list]).T, columns=['train_loss','test_loss','train_ACC','test_ACC','lr'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZRyjpmmj5CO"},"outputs":[],"source":["if not os.path.isdir(f'checkpoint/{name}'):\n","    os.mkdir(f'checkpoint/{name}')\n","result.to_csv(f'./checkpoint/{name}/result.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DL8FQYeNMKb8"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"veSaQT9GMK4h"},"source":["# Model_2_2 (Decay lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1668749008656,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"id":"KP6vd4J8MK4i","outputId":"561e4fcd-9d2b-4c2c-9354-8f9230438e9d"},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","ResNet                                   [10, 10]                  --\n","├─Conv2d: 1-1                            [10, 64, 32, 32]          1,728\n","├─BatchNorm2d: 1-2                       [10, 64, 32, 32]          128\n","├─Sequential: 1-3                        [10, 256, 32, 32]         --\n","│    └─Bottleneck: 2-1                   [10, 256, 32, 32]         --\n","│    │    └─Conv2d: 3-1                  [10, 64, 32, 32]          4,096\n","│    │    └─BatchNorm2d: 3-2             [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-3                  [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-4             [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-5                  [10, 256, 32, 32]         16,384\n","│    │    └─BatchNorm2d: 3-6             [10, 256, 32, 32]         512\n","│    │    └─Sequential: 3-7              [10, 256, 32, 32]         16,896\n","│    └─Bottleneck: 2-2                   [10, 256, 32, 32]         --\n","│    │    └─Conv2d: 3-8                  [10, 64, 32, 32]          16,384\n","│    │    └─BatchNorm2d: 3-9             [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-10                 [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-11            [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-12                 [10, 256, 32, 32]         16,384\n","│    │    └─BatchNorm2d: 3-13            [10, 256, 32, 32]         512\n","│    │    └─Sequential: 3-14             [10, 256, 32, 32]         --\n","│    └─Bottleneck: 2-3                   [10, 256, 32, 32]         --\n","│    │    └─Conv2d: 3-15                 [10, 64, 32, 32]          16,384\n","│    │    └─BatchNorm2d: 3-16            [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-17                 [10, 64, 32, 32]          36,864\n","│    │    └─BatchNorm2d: 3-18            [10, 64, 32, 32]          128\n","│    │    └─Conv2d: 3-19                 [10, 256, 32, 32]         16,384\n","│    │    └─BatchNorm2d: 3-20            [10, 256, 32, 32]         512\n","│    │    └─Sequential: 3-21             [10, 256, 32, 32]         --\n","├─Sequential: 1-4                        [10, 512, 16, 16]         --\n","│    └─Bottleneck: 2-4                   [10, 512, 16, 16]         --\n","│    │    └─Conv2d: 3-22                 [10, 128, 32, 32]         32,768\n","│    │    └─BatchNorm2d: 3-23            [10, 128, 32, 32]         256\n","│    │    └─Conv2d: 3-24                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-25            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-26                 [10, 512, 16, 16]         65,536\n","│    │    └─BatchNorm2d: 3-27            [10, 512, 16, 16]         1,024\n","│    │    └─Sequential: 3-28             [10, 512, 16, 16]         132,096\n","│    └─Bottleneck: 2-5                   [10, 512, 16, 16]         --\n","│    │    └─Conv2d: 3-29                 [10, 128, 16, 16]         65,536\n","│    │    └─BatchNorm2d: 3-30            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-31                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-32            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-33                 [10, 512, 16, 16]         65,536\n","│    │    └─BatchNorm2d: 3-34            [10, 512, 16, 16]         1,024\n","│    │    └─Sequential: 3-35             [10, 512, 16, 16]         --\n","│    └─Bottleneck: 2-6                   [10, 512, 16, 16]         --\n","│    │    └─Conv2d: 3-36                 [10, 128, 16, 16]         65,536\n","│    │    └─BatchNorm2d: 3-37            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-38                 [10, 128, 16, 16]         147,456\n","│    │    └─BatchNorm2d: 3-39            [10, 128, 16, 16]         256\n","│    │    └─Conv2d: 3-40                 [10, 512, 16, 16]         65,536\n","│    │    └─BatchNorm2d: 3-41            [10, 512, 16, 16]         1,024\n","│    │    └─Sequential: 3-42             [10, 512, 16, 16]         --\n","├─Sequential: 1-5                        [10, 1024, 8, 8]          --\n","│    └─Bottleneck: 2-7                   [10, 1024, 8, 8]          --\n","│    │    └─Conv2d: 3-43                 [10, 256, 16, 16]         131,072\n","│    │    └─BatchNorm2d: 3-44            [10, 256, 16, 16]         512\n","│    │    └─Conv2d: 3-45                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-46            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-47                 [10, 1024, 8, 8]          262,144\n","│    │    └─BatchNorm2d: 3-48            [10, 1024, 8, 8]          2,048\n","│    │    └─Sequential: 3-49             [10, 1024, 8, 8]          526,336\n","│    └─Bottleneck: 2-8                   [10, 1024, 8, 8]          --\n","│    │    └─Conv2d: 3-50                 [10, 256, 8, 8]           262,144\n","│    │    └─BatchNorm2d: 3-51            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-52                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-53            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-54                 [10, 1024, 8, 8]          262,144\n","│    │    └─BatchNorm2d: 3-55            [10, 1024, 8, 8]          2,048\n","│    │    └─Sequential: 3-56             [10, 1024, 8, 8]          --\n","│    └─Bottleneck: 2-9                   [10, 1024, 8, 8]          --\n","│    │    └─Conv2d: 3-57                 [10, 256, 8, 8]           262,144\n","│    │    └─BatchNorm2d: 3-58            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-59                 [10, 256, 8, 8]           589,824\n","│    │    └─BatchNorm2d: 3-60            [10, 256, 8, 8]           512\n","│    │    └─Conv2d: 3-61                 [10, 1024, 8, 8]          262,144\n","│    │    └─BatchNorm2d: 3-62            [10, 1024, 8, 8]          2,048\n","│    │    └─Sequential: 3-63             [10, 1024, 8, 8]          --\n","├─AdaptiveAvgPool2d: 1-6                 [10, 1024, 1, 1]          --\n","├─Linear: 1-7                            [10, 10]                  10,250\n","==========================================================================================\n","Total params: 4,914,250\n","Trainable params: 4,914,250\n","Non-trainable params: 0\n","Total mult-adds (G): 7.48\n","==========================================================================================\n","Input size (MB): 0.12\n","Forward/backward pass size (MB): 437.78\n","Params size (MB): 19.66\n","Estimated Total Size (MB): 457.56\n","=========================================================================================="]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["summary(ResNet_custom('model_2_2'), (10,3, 32, 32), depth=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"c67P_qHBMK4i","outputId":"9a21e7b6-e43d-44c2-b0e4-052b18778f49"},"outputs":[{"name":"stdout","output_type":"stream","text":["==> Building model..\n","\n","Epoch: 0\n","390 391 Loss: 2.225 | Acc: 22.930% (11465/50000)\n","99 100 Loss: 1.787 | Acc: 31.170% (3117/10000)\n","Learning Rate: 0.100000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 1\n","390 391 Loss: 1.699 | Acc: 34.772% (17386/50000)\n","99 100 Loss: 1.650 | Acc: 37.000% (3700/10000)\n","Learning Rate: 0.099975 | Epoch Time: 1 m 44 s\n","\n","Epoch: 2\n","390 391 Loss: 1.494 | Acc: 44.486% (22243/50000)\n","99 100 Loss: 1.444 | Acc: 47.600% (4760/10000)\n","Learning Rate: 0.099901 | Epoch Time: 1 m 44 s\n","\n","Epoch: 3\n","390 391 Loss: 1.295 | Acc: 52.668% (26334/50000)\n","99 100 Loss: 1.261 | Acc: 55.750% (5575/10000)\n","Learning Rate: 0.099778 | Epoch Time: 1 m 44 s\n","\n","Epoch: 4\n","390 391 Loss: 1.089 | Acc: 60.740% (30370/50000)\n","99 100 Loss: 1.159 | Acc: 58.190% (5819/10000)\n","Learning Rate: 0.099606 | Epoch Time: 1 m 44 s\n","\n","Epoch: 5\n","390 391 Loss: 0.957 | Acc: 65.696% (32848/50000)\n","99 100 Loss: 1.257 | Acc: 58.460% (5846/10000)\n","Learning Rate: 0.099384 | Epoch Time: 1 m 44 s\n","\n","Epoch: 6\n","390 391 Loss: 0.876 | Acc: 68.926% (34463/50000)\n","99 100 Loss: 1.075 | Acc: 63.130% (6313/10000)\n","Learning Rate: 0.099114 | Epoch Time: 1 m 44 s\n","\n","Epoch: 7\n","390 391 Loss: 0.798 | Acc: 71.874% (35937/50000)\n","99 100 Loss: 1.311 | Acc: 60.590% (6059/10000)\n","Learning Rate: 0.098796 | Epoch Time: 1 m 44 s\n","\n","Epoch: 8\n","390 391 Loss: 0.743 | Acc: 74.196% (37098/50000)\n","99 100 Loss: 0.920 | Acc: 68.390% (6839/10000)\n","Learning Rate: 0.098429 | Epoch Time: 1 m 44 s\n","\n","Epoch: 9\n","390 391 Loss: 0.707 | Acc: 75.360% (37680/50000)\n","99 100 Loss: 0.884 | Acc: 69.430% (6943/10000)\n","Learning Rate: 0.098015 | Epoch Time: 1 m 44 s\n","\n","Epoch: 10\n","390 391 Loss: 0.667 | Acc: 76.584% (38292/50000)\n","99 100 Loss: 1.233 | Acc: 61.630% (6163/10000)\n","Learning Rate: 0.097553 | Epoch Time: 1 m 44 s\n","\n","Epoch: 11\n","390 391 Loss: 0.641 | Acc: 77.732% (38866/50000)\n","99 100 Loss: 0.991 | Acc: 68.460% (6846/10000)\n","Learning Rate: 0.097044 | Epoch Time: 1 m 44 s\n","\n","Epoch: 12\n","390 391 Loss: 0.613 | Acc: 78.878% (39439/50000)\n","99 100 Loss: 0.990 | Acc: 67.850% (6785/10000)\n","Learning Rate: 0.096489 | Epoch Time: 1 m 44 s\n","\n","Epoch: 13\n","390 391 Loss: 0.592 | Acc: 79.468% (39734/50000)\n","99 100 Loss: 0.925 | Acc: 70.460% (7046/10000)\n","Learning Rate: 0.095888 | Epoch Time: 1 m 44 s\n","\n","Epoch: 14\n","390 391 Loss: 0.575 | Acc: 80.020% (40010/50000)\n","99 100 Loss: 0.738 | Acc: 75.390% (7539/10000)\n","Learning Rate: 0.095241 | Epoch Time: 1 m 44 s\n","\n","Epoch: 15\n","390 391 Loss: 0.553 | Acc: 80.748% (40374/50000)\n","99 100 Loss: 1.028 | Acc: 70.300% (7030/10000)\n","Learning Rate: 0.094550 | Epoch Time: 1 m 44 s\n","\n","Epoch: 16\n","390 391 Loss: 0.533 | Acc: 81.624% (40812/50000)\n","99 100 Loss: 1.030 | Acc: 68.670% (6867/10000)\n","Learning Rate: 0.093815 | Epoch Time: 1 m 44 s\n","\n","Epoch: 17\n","390 391 Loss: 0.520 | Acc: 82.254% (41127/50000)\n","99 100 Loss: 0.867 | Acc: 72.160% (7216/10000)\n","Learning Rate: 0.093037 | Epoch Time: 1 m 44 s\n","\n","Epoch: 18\n","390 391 Loss: 0.505 | Acc: 82.542% (41271/50000)\n","99 100 Loss: 0.735 | Acc: 76.160% (7616/10000)\n","Learning Rate: 0.092216 | Epoch Time: 1 m 44 s\n","\n","Epoch: 19\n","390 391 Loss: 0.500 | Acc: 82.798% (41399/50000)\n","99 100 Loss: 0.751 | Acc: 74.880% (7488/10000)\n","Learning Rate: 0.091354 | Epoch Time: 1 m 44 s\n","\n","Epoch: 20\n","390 391 Loss: 0.477 | Acc: 83.386% (41693/50000)\n","99 100 Loss: 0.705 | Acc: 77.770% (7777/10000)\n","Learning Rate: 0.090451 | Epoch Time: 1 m 44 s\n","\n","Epoch: 21\n","390 391 Loss: 0.477 | Acc: 83.492% (41746/50000)\n","99 100 Loss: 0.636 | Acc: 78.810% (7881/10000)\n","Learning Rate: 0.089508 | Epoch Time: 1 m 44 s\n","\n","Epoch: 22\n","390 391 Loss: 0.467 | Acc: 83.768% (41884/50000)\n","99 100 Loss: 0.692 | Acc: 77.800% (7780/10000)\n","Learning Rate: 0.088526 | Epoch Time: 1 m 44 s\n","\n","Epoch: 23\n","390 391 Loss: 0.458 | Acc: 84.142% (42071/50000)\n","99 100 Loss: 0.678 | Acc: 77.790% (7779/10000)\n","Learning Rate: 0.087506 | Epoch Time: 1 m 44 s\n","\n","Epoch: 24\n","390 391 Loss: 0.455 | Acc: 84.378% (42189/50000)\n","99 100 Loss: 0.719 | Acc: 76.390% (7639/10000)\n","Learning Rate: 0.086448 | Epoch Time: 1 m 44 s\n","\n","Epoch: 25\n","390 391 Loss: 0.441 | Acc: 84.916% (42458/50000)\n","99 100 Loss: 0.887 | Acc: 74.170% (7417/10000)\n","Learning Rate: 0.085355 | Epoch Time: 1 m 44 s\n","\n","Epoch: 26\n","390 391 Loss: 0.427 | Acc: 85.232% (42616/50000)\n","99 100 Loss: 0.588 | Acc: 80.280% (8028/10000)\n","Learning Rate: 0.084227 | Epoch Time: 1 m 44 s\n","\n","Epoch: 27\n","390 391 Loss: 0.428 | Acc: 85.124% (42562/50000)\n","99 100 Loss: 0.692 | Acc: 77.210% (7721/10000)\n","Learning Rate: 0.083066 | Epoch Time: 1 m 44 s\n","\n","Epoch: 28\n","390 391 Loss: 0.423 | Acc: 85.368% (42684/50000)\n","99 100 Loss: 0.499 | Acc: 83.120% (8312/10000)\n","Learning Rate: 0.081871 | Epoch Time: 1 m 44 s\n","\n","Epoch: 29\n","390 391 Loss: 0.418 | Acc: 85.660% (42830/50000)\n","99 100 Loss: 0.646 | Acc: 78.650% (7865/10000)\n","Learning Rate: 0.080645 | Epoch Time: 1 m 44 s\n","\n","Epoch: 30\n","390 391 Loss: 0.402 | Acc: 86.080% (43040/50000)\n","99 100 Loss: 0.661 | Acc: 78.240% (7824/10000)\n","Learning Rate: 0.079389 | Epoch Time: 1 m 44 s\n","\n","Epoch: 31\n","390 391 Loss: 0.396 | Acc: 86.348% (43174/50000)\n","99 100 Loss: 0.719 | Acc: 78.310% (7831/10000)\n","Learning Rate: 0.078104 | Epoch Time: 1 m 44 s\n","\n","Epoch: 32\n","390 391 Loss: 0.390 | Acc: 86.540% (43270/50000)\n","99 100 Loss: 0.591 | Acc: 80.510% (8051/10000)\n","Learning Rate: 0.076791 | Epoch Time: 1 m 44 s\n","\n","Epoch: 33\n","390 391 Loss: 0.389 | Acc: 86.562% (43281/50000)\n","99 100 Loss: 0.854 | Acc: 74.540% (7454/10000)\n","Learning Rate: 0.075452 | Epoch Time: 1 m 44 s\n","\n","Epoch: 34\n","390 391 Loss: 0.376 | Acc: 87.148% (43574/50000)\n","99 100 Loss: 0.542 | Acc: 81.580% (8158/10000)\n","Learning Rate: 0.074088 | Epoch Time: 1 m 44 s\n","\n","Epoch: 35\n","390 391 Loss: 0.375 | Acc: 86.976% (43488/50000)\n","99 100 Loss: 0.561 | Acc: 81.590% (8159/10000)\n","Learning Rate: 0.072700 | Epoch Time: 1 m 44 s\n","\n","Epoch: 36\n","390 391 Loss: 0.365 | Acc: 87.466% (43733/50000)\n","99 100 Loss: 0.611 | Acc: 79.640% (7964/10000)\n","Learning Rate: 0.071289 | Epoch Time: 1 m 44 s\n","\n","Epoch: 37\n","390 391 Loss: 0.359 | Acc: 87.558% (43779/50000)\n","99 100 Loss: 0.481 | Acc: 83.950% (8395/10000)\n","Learning Rate: 0.069857 | Epoch Time: 1 m 44 s\n","\n","Epoch: 38\n","390 391 Loss: 0.356 | Acc: 87.726% (43863/50000)\n","99 100 Loss: 0.488 | Acc: 83.210% (8321/10000)\n","Learning Rate: 0.068406 | Epoch Time: 1 m 44 s\n","\n","Epoch: 39\n","390 391 Loss: 0.345 | Acc: 87.908% (43954/50000)\n","99 100 Loss: 0.466 | Acc: 84.490% (8449/10000)\n","Learning Rate: 0.066937 | Epoch Time: 1 m 44 s\n","\n","Epoch: 40\n","390 391 Loss: 0.342 | Acc: 88.198% (44099/50000)\n","99 100 Loss: 0.487 | Acc: 83.740% (8374/10000)\n","Learning Rate: 0.065451 | Epoch Time: 1 m 44 s\n","\n","Epoch: 41\n","390 391 Loss: 0.336 | Acc: 88.294% (44147/50000)\n","99 100 Loss: 0.551 | Acc: 82.340% (8234/10000)\n","Learning Rate: 0.063950 | Epoch Time: 1 m 44 s\n","\n","Epoch: 42\n","390 391 Loss: 0.327 | Acc: 88.692% (44346/50000)\n","99 100 Loss: 0.538 | Acc: 82.690% (8269/10000)\n","Learning Rate: 0.062434 | Epoch Time: 1 m 44 s\n","\n","Epoch: 43\n","390 391 Loss: 0.322 | Acc: 88.850% (44425/50000)\n","99 100 Loss: 0.479 | Acc: 84.570% (8457/10000)\n","Learning Rate: 0.060907 | Epoch Time: 1 m 44 s\n","\n","Epoch: 44\n","390 391 Loss: 0.310 | Acc: 89.236% (44618/50000)\n","99 100 Loss: 0.506 | Acc: 83.560% (8356/10000)\n","Learning Rate: 0.059369 | Epoch Time: 1 m 44 s\n","\n","Epoch: 45\n","390 391 Loss: 0.303 | Acc: 89.536% (44768/50000)\n","99 100 Loss: 0.935 | Acc: 74.250% (7425/10000)\n","Learning Rate: 0.057822 | Epoch Time: 1 m 44 s\n","\n","Epoch: 46\n","390 391 Loss: 0.308 | Acc: 89.446% (44723/50000)\n","99 100 Loss: 0.471 | Acc: 84.790% (8479/10000)\n","Learning Rate: 0.056267 | Epoch Time: 1 m 44 s\n","\n","Epoch: 47\n","390 391 Loss: 0.295 | Acc: 89.912% (44956/50000)\n","99 100 Loss: 0.610 | Acc: 80.250% (8025/10000)\n","Learning Rate: 0.054705 | Epoch Time: 1 m 44 s\n","\n","Epoch: 48\n","390 391 Loss: 0.287 | Acc: 90.124% (45062/50000)\n","99 100 Loss: 0.414 | Acc: 85.660% (8566/10000)\n","Learning Rate: 0.053140 | Epoch Time: 1 m 44 s\n","\n","Epoch: 49\n","390 391 Loss: 0.281 | Acc: 90.226% (45113/50000)\n","99 100 Loss: 0.475 | Acc: 84.630% (8463/10000)\n","Learning Rate: 0.051571 | Epoch Time: 1 m 44 s\n","\n","Epoch: 50\n","390 391 Loss: 0.276 | Acc: 90.424% (45212/50000)\n","99 100 Loss: 0.440 | Acc: 85.490% (8549/10000)\n","Learning Rate: 0.050000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 51\n","390 391 Loss: 0.267 | Acc: 90.852% (45426/50000)\n","99 100 Loss: 0.492 | Acc: 84.470% (8447/10000)\n","Learning Rate: 0.048429 | Epoch Time: 1 m 44 s\n","\n","Epoch: 52\n","390 391 Loss: 0.261 | Acc: 91.000% (45500/50000)\n","99 100 Loss: 0.398 | Acc: 86.970% (8697/10000)\n","Learning Rate: 0.046860 | Epoch Time: 1 m 44 s\n","\n","Epoch: 53\n","390 391 Loss: 0.254 | Acc: 91.230% (45615/50000)\n","99 100 Loss: 0.772 | Acc: 78.640% (7864/10000)\n","Learning Rate: 0.045295 | Epoch Time: 1 m 44 s\n","\n","Epoch: 54\n","390 391 Loss: 0.247 | Acc: 91.488% (45744/50000)\n","99 100 Loss: 0.433 | Acc: 86.200% (8620/10000)\n","Learning Rate: 0.043733 | Epoch Time: 1 m 44 s\n","\n","Epoch: 55\n","390 391 Loss: 0.236 | Acc: 91.940% (45970/50000)\n","99 100 Loss: 0.427 | Acc: 86.110% (8611/10000)\n","Learning Rate: 0.042178 | Epoch Time: 1 m 44 s\n","\n","Epoch: 56\n","390 391 Loss: 0.229 | Acc: 92.084% (46042/50000)\n","99 100 Loss: 0.524 | Acc: 83.530% (8353/10000)\n","Learning Rate: 0.040631 | Epoch Time: 1 m 44 s\n","\n","Epoch: 57\n","390 391 Loss: 0.226 | Acc: 92.178% (46089/50000)\n","99 100 Loss: 0.434 | Acc: 85.840% (8584/10000)\n","Learning Rate: 0.039093 | Epoch Time: 1 m 44 s\n","\n","Epoch: 58\n","390 391 Loss: 0.218 | Acc: 92.556% (46278/50000)\n","99 100 Loss: 0.507 | Acc: 84.380% (8438/10000)\n","Learning Rate: 0.037566 | Epoch Time: 1 m 44 s\n","\n","Epoch: 59\n","390 391 Loss: 0.208 | Acc: 92.846% (46423/50000)\n","99 100 Loss: 0.375 | Acc: 87.610% (8761/10000)\n","Learning Rate: 0.036050 | Epoch Time: 1 m 44 s\n","\n","Epoch: 60\n","390 391 Loss: 0.199 | Acc: 93.094% (46547/50000)\n","99 100 Loss: 0.545 | Acc: 83.480% (8348/10000)\n","Learning Rate: 0.034549 | Epoch Time: 1 m 44 s\n","\n","Epoch: 61\n","390 391 Loss: 0.191 | Acc: 93.324% (46662/50000)\n","99 100 Loss: 0.416 | Acc: 87.570% (8757/10000)\n","Learning Rate: 0.033063 | Epoch Time: 1 m 44 s\n","\n","Epoch: 62\n","390 391 Loss: 0.190 | Acc: 93.354% (46677/50000)\n","99 100 Loss: 0.491 | Acc: 85.090% (8509/10000)\n","Learning Rate: 0.031594 | Epoch Time: 1 m 44 s\n","\n","Epoch: 63\n","390 391 Loss: 0.180 | Acc: 93.714% (46857/50000)\n","99 100 Loss: 0.416 | Acc: 87.080% (8708/10000)\n","Learning Rate: 0.030143 | Epoch Time: 1 m 44 s\n","\n","Epoch: 64\n","390 391 Loss: 0.170 | Acc: 94.148% (47074/50000)\n","99 100 Loss: 0.320 | Acc: 89.300% (8930/10000)\n","Learning Rate: 0.028711 | Epoch Time: 1 m 44 s\n","\n","Epoch: 65\n","390 391 Loss: 0.155 | Acc: 94.714% (47357/50000)\n","99 100 Loss: 0.363 | Acc: 88.110% (8811/10000)\n","Learning Rate: 0.027300 | Epoch Time: 1 m 44 s\n","\n","Epoch: 66\n","390 391 Loss: 0.149 | Acc: 94.876% (47438/50000)\n","99 100 Loss: 0.413 | Acc: 87.570% (8757/10000)\n","Learning Rate: 0.025912 | Epoch Time: 1 m 44 s\n","\n","Epoch: 67\n","390 391 Loss: 0.147 | Acc: 94.984% (47492/50000)\n","99 100 Loss: 0.311 | Acc: 90.340% (9034/10000)\n","Learning Rate: 0.024548 | Epoch Time: 1 m 44 s\n","\n","Epoch: 68\n","390 391 Loss: 0.134 | Acc: 95.438% (47719/50000)\n","99 100 Loss: 0.306 | Acc: 90.590% (9059/10000)\n","Learning Rate: 0.023209 | Epoch Time: 1 m 44 s\n","\n","Epoch: 69\n","390 391 Loss: 0.124 | Acc: 95.794% (47897/50000)\n","99 100 Loss: 0.376 | Acc: 88.620% (8862/10000)\n","Learning Rate: 0.021896 | Epoch Time: 1 m 44 s\n","\n","Epoch: 70\n","390 391 Loss: 0.117 | Acc: 96.062% (48031/50000)\n","99 100 Loss: 0.300 | Acc: 90.530% (9053/10000)\n","Learning Rate: 0.020611 | Epoch Time: 1 m 44 s\n","\n","Epoch: 71\n","390 391 Loss: 0.106 | Acc: 96.456% (48228/50000)\n","99 100 Loss: 0.299 | Acc: 90.960% (9096/10000)\n","Learning Rate: 0.019355 | Epoch Time: 1 m 44 s\n","\n","Epoch: 72\n","390 391 Loss: 0.100 | Acc: 96.612% (48306/50000)\n","99 100 Loss: 0.308 | Acc: 90.120% (9012/10000)\n","Learning Rate: 0.018129 | Epoch Time: 1 m 44 s\n","\n","Epoch: 73\n","390 391 Loss: 0.091 | Acc: 96.932% (48466/50000)\n","99 100 Loss: 0.288 | Acc: 91.040% (9104/10000)\n","Learning Rate: 0.016934 | Epoch Time: 1 m 44 s\n","\n","Epoch: 74\n","390 391 Loss: 0.081 | Acc: 97.388% (48694/50000)\n","99 100 Loss: 0.265 | Acc: 91.870% (9187/10000)\n","Learning Rate: 0.015773 | Epoch Time: 1 m 44 s\n","\n","Epoch: 75\n","390 391 Loss: 0.071 | Acc: 97.646% (48823/50000)\n","99 100 Loss: 0.300 | Acc: 91.200% (9120/10000)\n","Learning Rate: 0.014645 | Epoch Time: 1 m 44 s\n","\n","Epoch: 76\n","390 391 Loss: 0.066 | Acc: 97.862% (48931/50000)\n","99 100 Loss: 0.269 | Acc: 92.030% (9203/10000)\n","Learning Rate: 0.013552 | Epoch Time: 1 m 44 s\n","\n","Epoch: 77\n","390 391 Loss: 0.054 | Acc: 98.314% (49157/50000)\n","99 100 Loss: 0.294 | Acc: 91.330% (9133/10000)\n","Learning Rate: 0.012494 | Epoch Time: 1 m 44 s\n","\n","Epoch: 78\n","390 391 Loss: 0.047 | Acc: 98.594% (49297/50000)\n","99 100 Loss: 0.260 | Acc: 92.400% (9240/10000)\n","Learning Rate: 0.011474 | Epoch Time: 1 m 44 s\n","\n","Epoch: 79\n","390 391 Loss: 0.040 | Acc: 98.790% (49395/50000)\n","99 100 Loss: 0.232 | Acc: 93.230% (9323/10000)\n","Learning Rate: 0.010492 | Epoch Time: 1 m 44 s\n","\n","Epoch: 80\n","390 391 Loss: 0.033 | Acc: 99.062% (49531/50000)\n","99 100 Loss: 0.256 | Acc: 92.700% (9270/10000)\n","Learning Rate: 0.009549 | Epoch Time: 1 m 44 s\n","\n","Epoch: 81\n","390 391 Loss: 0.029 | Acc: 99.238% (49619/50000)\n","99 100 Loss: 0.252 | Acc: 92.800% (9280/10000)\n","Learning Rate: 0.008646 | Epoch Time: 1 m 44 s\n","\n","Epoch: 82\n","390 391 Loss: 0.022 | Acc: 99.492% (49746/50000)\n","99 100 Loss: 0.223 | Acc: 93.570% (9357/10000)\n","Learning Rate: 0.007784 | Epoch Time: 1 m 44 s\n","\n","Epoch: 83\n","390 391 Loss: 0.018 | Acc: 99.622% (49811/50000)\n","99 100 Loss: 0.218 | Acc: 93.770% (9377/10000)\n","Learning Rate: 0.006963 | Epoch Time: 1 m 44 s\n","\n","Epoch: 84\n","390 391 Loss: 0.015 | Acc: 99.714% (49857/50000)\n","99 100 Loss: 0.227 | Acc: 93.730% (9373/10000)\n","Learning Rate: 0.006185 | Epoch Time: 1 m 44 s\n","\n","Epoch: 85\n","390 391 Loss: 0.013 | Acc: 99.784% (49892/50000)\n","99 100 Loss: 0.215 | Acc: 93.900% (9390/10000)\n","Learning Rate: 0.005450 | Epoch Time: 1 m 44 s\n","\n","Epoch: 86\n","390 391 Loss: 0.011 | Acc: 99.844% (49922/50000)\n","99 100 Loss: 0.213 | Acc: 93.970% (9397/10000)\n","Learning Rate: 0.004759 | Epoch Time: 1 m 44 s\n","\n","Epoch: 87\n","390 391 Loss: 0.009 | Acc: 99.884% (49942/50000)\n","99 100 Loss: 0.210 | Acc: 94.130% (9413/10000)\n","Learning Rate: 0.004112 | Epoch Time: 1 m 44 s\n","\n","Epoch: 88\n","390 391 Loss: 0.009 | Acc: 99.888% (49944/50000)\n","99 100 Loss: 0.210 | Acc: 94.060% (9406/10000)\n","Learning Rate: 0.003511 | Epoch Time: 1 m 44 s\n","\n","Epoch: 89\n","390 391 Loss: 0.007 | Acc: 99.942% (49971/50000)\n","99 100 Loss: 0.209 | Acc: 94.030% (9403/10000)\n","Learning Rate: 0.002956 | Epoch Time: 1 m 44 s\n","\n","Epoch: 90\n","390 391 Loss: 0.007 | Acc: 99.942% (49971/50000)\n","99 100 Loss: 0.210 | Acc: 93.950% (9395/10000)\n","Learning Rate: 0.002447 | Epoch Time: 1 m 44 s\n","\n","Epoch: 91\n","390 391 Loss: 0.007 | Acc: 99.922% (49961/50000)\n","99 100 Loss: 0.207 | Acc: 94.050% (9405/10000)\n","Learning Rate: 0.001985 | Epoch Time: 1 m 44 s\n","\n","Epoch: 92\n","390 391 Loss: 0.006 | Acc: 99.954% (49977/50000)\n","99 100 Loss: 0.205 | Acc: 94.150% (9415/10000)\n","Learning Rate: 0.001571 | Epoch Time: 1 m 44 s\n","\n","Epoch: 93\n","390 391 Loss: 0.006 | Acc: 99.948% (49974/50000)\n","99 100 Loss: 0.206 | Acc: 94.190% (9419/10000)\n","Learning Rate: 0.001204 | Epoch Time: 1 m 44 s\n","\n","Epoch: 94\n","390 391 Loss: 0.006 | Acc: 99.950% (49975/50000)\n","99 100 Loss: 0.205 | Acc: 94.090% (9409/10000)\n","Learning Rate: 0.000886 | Epoch Time: 1 m 44 s\n","\n","Epoch: 95\n","390 391 Loss: 0.006 | Acc: 99.954% (49977/50000)\n","99 100 Loss: 0.206 | Acc: 94.110% (9411/10000)\n","Learning Rate: 0.000616 | Epoch Time: 1 m 44 s\n","\n","Epoch: 96\n","390 391 Loss: 0.006 | Acc: 99.962% (49981/50000)\n","99 100 Loss: 0.208 | Acc: 94.060% (9406/10000)\n","Learning Rate: 0.000394 | Epoch Time: 1 m 44 s\n","\n","Epoch: 97\n","390 391 Loss: 0.005 | Acc: 99.978% (49989/50000)\n","99 100 Loss: 0.206 | Acc: 94.060% (9406/10000)\n","Learning Rate: 0.000222 | Epoch Time: 1 m 44 s\n","\n","Epoch: 98\n","390 391 Loss: 0.005 | Acc: 99.978% (49989/50000)\n","99 100 Loss: 0.205 | Acc: 94.030% (9403/10000)\n","Learning Rate: 0.000099 | Epoch Time: 1 m 44 s\n","\n","Epoch: 99\n","390 391 Loss: 0.005 | Acc: 99.970% (49985/50000)\n","99 100 Loss: 0.207 | Acc: 94.050% (9405/10000)\n","Learning Rate: 0.000025 | Epoch Time: 1 m 44 s\n"]}],"source":["# TODO: Change the model to your own selection\n","name = \"model_2_2\"\n","name = name.lower()\n","resume = None\n","# resume = f\"./checkpoint/{name}/001.pth\" # change the checkpoint name to the one desired\n","\n","# Model\n","print('==> Building model..')\n","net = ResNet_custom(name).to(device)\n","\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","if resume is not None:\n","    # Load checkpoint.\n","    print(f'==> Resuming from {resume}..')\n","    assert os.path.exists(resume), 'Error: no checkpoint found!'\n","    checkpoint = torch.load(resume)\n","    assert name == checkpoint['name'], 'Error: model does not match checkpoint!'\n","    net.load_state_dict(checkpoint['net'])\n","    best_acc = checkpoint['acc']\n","    start_epoch = checkpoint['epoch']\n","\n","# TODO: Loss\n","criterion = nn.CrossEntropyLoss()\n","# TODO: optimizer, SGD vs Adam, learning rate selection, etc...\n","optimizer = optim.SGD(net.parameters(), lr=0.1,\n","                      momentum=0.9, weight_decay=5e-4)\n","scheduler = torch.optim.lr_scheduler.余弦退火LR(optimizer, T_max=100)\n","\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","    train_loss_list.append(train_loss/(batch_idx+1))\n","    train_ACC_list.append(100.*correct/total)\n","\n","\n","    print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","        test_loss_list.append(test_loss/(batch_idx+1))\n","        test_ACC_list.append(100.*correct/total)\n","\n","        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'name': name,\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir(f'checkpoint/{name}'):\n","            os.mkdir(f'checkpoint/{name}')\n","        torch.save(state, f'./checkpoint/{name}/{epoch:03}.pth')\n","        best_acc = acc\n","\n","train_loss_list = []\n","test_loss_list = []\n","train_ACC_list = []\n","test_ACC_list = []\n","lr_list = []\n","\n","# TODO: decide when the training should stop\n","for epoch in range(start_epoch, start_epoch+100):\n","    start_time = time.time()\n","\n","    train(epoch)\n","    test(epoch)\n","\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(\"Learning Rate: %f | Epoch Time: %i m %i s\" % (optimizer.param_groups[0]['lr'], epoch_mins,epoch_secs))\n","\n","    lr_list.append(optimizer.param_groups[0]['lr'])\n","\n","    scheduler.step()\n","\n","    # break # for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"g13gOKelMK4i"},"outputs":[],"source":["result = pd.DataFrame(np.array([train_loss_list, test_loss_list,\n","                       train_ACC_list, test_ACC_list,\n","                       lr_list]).T, columns=['train_loss','test_loss','train_ACC','test_ACC','lr'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JSXHOaf7MK4i"},"outputs":[],"source":["if not os.path.isdir(f'checkpoint/{name}'):\n","    os.mkdir(f'checkpoint/{name}')\n","result.to_csv(f'./checkpoint/{name}/result.csv')"]},{"cell_type":"code","source":[],"metadata":{"id":"Zq7tq4UrU4UE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YQKT5uUGU4p5"},"source":["# Model_2_3 (Adam)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"8fdf8249-cd36-42d1-8a57-775ed5104955","id":"mrZy8sbWU4p6","executionInfo":{"status":"error","timestamp":1669087294155,"user_tz":300,"elapsed":2523730,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["==> Building model..\n","==> Resuming from /content/drive/MyDrive/DL_mini_project/checkpoint/model_2_3/053.pth..\n","\n","Epoch: 53\n","390 391 Loss: 0.071 | Acc: 97.552% (48776/50000)\n","99 100 Loss: 0.544 | Acc: 89.270% (8927/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 46 s\n","\n","Epoch: 54\n","390 391 Loss: 0.066 | Acc: 97.622% (48811/50000)\n","99 100 Loss: 0.542 | Acc: 88.610% (8861/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 55\n","390 391 Loss: 0.062 | Acc: 97.808% (48904/50000)\n","99 100 Loss: 0.541 | Acc: 88.960% (8896/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 56\n","390 391 Loss: 0.065 | Acc: 97.764% (48882/50000)\n","99 100 Loss: 0.542 | Acc: 88.420% (8842/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 57\n","390 391 Loss: 0.064 | Acc: 97.780% (48890/50000)\n","99 100 Loss: 0.526 | Acc: 89.250% (8925/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 58\n","390 391 Loss: 0.059 | Acc: 98.030% (49015/50000)\n","99 100 Loss: 0.491 | Acc: 89.270% (8927/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 59\n","390 391 Loss: 0.055 | Acc: 98.052% (49026/50000)\n","99 100 Loss: 0.538 | Acc: 89.210% (8921/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 60\n","390 391 Loss: 0.057 | Acc: 97.992% (48996/50000)\n","99 100 Loss: 0.550 | Acc: 89.270% (8927/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 61\n","390 391 Loss: 0.056 | Acc: 98.030% (49015/50000)\n","99 100 Loss: 0.616 | Acc: 88.820% (8882/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 62\n","390 391 Loss: 0.056 | Acc: 98.046% (49023/50000)\n","99 100 Loss: 0.509 | Acc: 89.950% (8995/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 63\n","390 391 Loss: 0.054 | Acc: 98.102% (49051/50000)\n","99 100 Loss: 0.651 | Acc: 87.860% (8786/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 64\n","390 391 Loss: 0.049 | Acc: 98.256% (49128/50000)\n","99 100 Loss: 0.530 | Acc: 89.420% (8942/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 65\n","390 391 Loss: 0.054 | Acc: 98.118% (49059/50000)\n","99 100 Loss: 0.599 | Acc: 88.270% (8827/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 66\n","390 391 Loss: 0.049 | Acc: 98.232% (49116/50000)\n","99 100 Loss: 0.572 | Acc: 89.210% (8921/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 67\n","390 391 Loss: 0.053 | Acc: 98.168% (49084/50000)\n","99 100 Loss: 0.595 | Acc: 89.130% (8913/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 68\n","390 391 Loss: 0.051 | Acc: 98.188% (49094/50000)\n","99 100 Loss: 0.549 | Acc: 89.370% (8937/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 69\n","390 391 Loss: 0.043 | Acc: 98.464% (49232/50000)\n","99 100 Loss: 0.592 | Acc: 89.020% (8902/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 70\n","390 391 Loss: 0.050 | Acc: 98.272% (49136/50000)\n","99 100 Loss: 0.515 | Acc: 89.790% (8979/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 71\n","390 391 Loss: 0.050 | Acc: 98.246% (49123/50000)\n","99 100 Loss: 0.552 | Acc: 89.410% (8941/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 72\n","390 391 Loss: 0.045 | Acc: 98.458% (49229/50000)\n","99 100 Loss: 0.509 | Acc: 89.850% (8985/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 73\n","390 391 Loss: 0.047 | Acc: 98.330% (49165/50000)\n","99 100 Loss: 0.727 | Acc: 87.690% (8769/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 74\n","390 391 Loss: 0.046 | Acc: 98.446% (49223/50000)\n","99 100 Loss: 0.672 | Acc: 88.290% (8829/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 75\n","390 391 Loss: 0.044 | Acc: 98.524% (49262/50000)\n","99 100 Loss: 0.507 | Acc: 90.390% (9039/10000)\n","Saving..\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 76\n","390 391 Loss: 0.044 | Acc: 98.492% (49246/50000)\n","99 100 Loss: 0.585 | Acc: 89.540% (8954/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 77\n","390 391 Loss: 0.044 | Acc: 98.508% (49254/50000)\n","99 100 Loss: 0.659 | Acc: 88.960% (8896/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 78\n","390 391 Loss: 0.046 | Acc: 98.372% (49186/50000)\n","99 100 Loss: 0.502 | Acc: 90.430% (9043/10000)\n","Saving..\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 79\n","390 391 Loss: 0.041 | Acc: 98.616% (49308/50000)\n","99 100 Loss: 0.648 | Acc: 88.320% (8832/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 80\n","390 391 Loss: 0.037 | Acc: 98.710% (49355/50000)\n","99 100 Loss: 0.561 | Acc: 90.150% (9015/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 81\n","390 391 Loss: 0.042 | Acc: 98.596% (49298/50000)\n","99 100 Loss: 0.597 | Acc: 89.520% (8952/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 82\n","390 391 Loss: 0.042 | Acc: 98.566% (49283/50000)\n","99 100 Loss: 0.614 | Acc: 89.030% (8903/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 83\n","390 391 Loss: 0.041 | Acc: 98.592% (49296/50000)\n","99 100 Loss: 0.595 | Acc: 89.490% (8949/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 84\n","390 391 Loss: 0.041 | Acc: 98.590% (49295/50000)\n","99 100 Loss: 0.644 | Acc: 89.190% (8919/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 85\n","390 391 Loss: 0.039 | Acc: 98.684% (49342/50000)\n","99 100 Loss: 0.592 | Acc: 89.550% (8955/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 86\n","390 391 Loss: 0.035 | Acc: 98.772% (49386/50000)\n","99 100 Loss: 0.571 | Acc: 90.020% (9002/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 87\n","390 391 Loss: 0.041 | Acc: 98.514% (49257/50000)\n","99 100 Loss: 0.561 | Acc: 89.950% (8995/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 88\n","390 391 Loss: 0.037 | Acc: 98.694% (49347/50000)\n","99 100 Loss: 0.574 | Acc: 89.690% (8969/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 89\n","390 391 Loss: 0.038 | Acc: 98.722% (49361/50000)\n","99 100 Loss: 0.680 | Acc: 88.940% (8894/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 90\n","390 391 Loss: 0.039 | Acc: 98.676% (49338/50000)\n","99 100 Loss: 0.614 | Acc: 89.500% (8950/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 91\n","390 391 Loss: 0.039 | Acc: 98.676% (49338/50000)\n","99 100 Loss: 0.561 | Acc: 90.330% (9033/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 92\n","390 391 Loss: 0.036 | Acc: 98.808% (49404/50000)\n","99 100 Loss: 0.619 | Acc: 89.610% (8961/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 93\n","390 391 Loss: 0.035 | Acc: 98.762% (49381/50000)\n","99 100 Loss: 0.554 | Acc: 90.090% (9009/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 94\n","390 391 Loss: 0.035 | Acc: 98.822% (49411/50000)\n","99 100 Loss: 0.556 | Acc: 90.060% (9006/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 95\n","390 391 Loss: 0.035 | Acc: 98.806% (49403/50000)\n","99 100 Loss: 0.624 | Acc: 89.650% (8965/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 96\n","390 391 Loss: 0.040 | Acc: 98.640% (49320/50000)\n","99 100 Loss: 0.607 | Acc: 89.990% (8999/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 97\n","390 391 Loss: 0.032 | Acc: 98.910% (49455/50000)\n","99 100 Loss: 0.576 | Acc: 89.940% (8994/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 98\n","390 391 Loss: 0.032 | Acc: 98.966% (49483/50000)\n","99 100 Loss: 0.601 | Acc: 90.350% (9035/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 99\n","390 391 Loss: 0.036 | Acc: 98.810% (49405/50000)\n","99 100 Loss: 0.606 | Acc: 89.550% (8955/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 100\n","390 391 Loss: 0.035 | Acc: 98.780% (49390/50000)\n","99 100 Loss: 0.638 | Acc: 88.930% (8893/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 101\n","390 391 Loss: 0.039 | Acc: 98.682% (49341/50000)\n","99 100 Loss: 0.601 | Acc: 89.850% (8985/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 102\n","390 391 Loss: 0.030 | Acc: 98.938% (49469/50000)\n","99 100 Loss: 0.645 | Acc: 89.500% (8950/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 103\n","390 391 Loss: 0.037 | Acc: 98.778% (49389/50000)\n","99 100 Loss: 0.568 | Acc: 90.030% (9003/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 104\n","390 391 Loss: 0.028 | Acc: 99.054% (49527/50000)\n","99 100 Loss: 0.621 | Acc: 89.860% (8986/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 105\n","390 391 Loss: 0.034 | Acc: 98.876% (49438/50000)\n","99 100 Loss: 0.605 | Acc: 89.750% (8975/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 106\n","390 391 Loss: 0.030 | Acc: 98.984% (49492/50000)\n","99 100 Loss: 0.598 | Acc: 90.440% (9044/10000)\n","Saving..\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 107\n","390 391 Loss: 0.031 | Acc: 98.922% (49461/50000)\n","99 100 Loss: 0.708 | Acc: 89.590% (8959/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 108\n","390 391 Loss: 0.032 | Acc: 98.922% (49461/50000)\n","99 100 Loss: 0.580 | Acc: 90.090% (9009/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 109\n","390 391 Loss: 0.030 | Acc: 98.970% (49485/50000)\n","99 100 Loss: 0.638 | Acc: 90.130% (9013/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 110\n","390 391 Loss: 0.031 | Acc: 98.932% (49466/50000)\n","99 100 Loss: 0.627 | Acc: 89.920% (8992/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 111\n","390 391 Loss: 0.032 | Acc: 98.902% (49451/50000)\n","99 100 Loss: 0.604 | Acc: 90.210% (9021/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 112\n","390 391 Loss: 0.033 | Acc: 98.880% (49440/50000)\n","99 100 Loss: 0.565 | Acc: 90.560% (9056/10000)\n","Saving..\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 113\n","390 391 Loss: 0.026 | Acc: 99.080% (49540/50000)\n","99 100 Loss: 0.612 | Acc: 89.710% (8971/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 114\n","390 391 Loss: 0.032 | Acc: 98.910% (49455/50000)\n","99 100 Loss: 0.613 | Acc: 90.050% (9005/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 115\n","390 391 Loss: 0.024 | Acc: 99.196% (49598/50000)\n","99 100 Loss: 0.610 | Acc: 90.080% (9008/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 116\n","390 391 Loss: 0.035 | Acc: 98.750% (49375/50000)\n","99 100 Loss: 0.713 | Acc: 88.950% (8895/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 117\n","390 391 Loss: 0.030 | Acc: 98.968% (49484/50000)\n","99 100 Loss: 0.601 | Acc: 90.380% (9038/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 44 s\n","\n","Epoch: 118\n","390 391 Loss: 0.028 | Acc: 99.036% (49518/50000)\n","99 100 Loss: 0.699 | Acc: 89.790% (8979/10000)\n","Learning Rate: 0.010000 | Epoch Time: 1 m 45 s\n","\n","Epoch: 119\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-082c5f9630d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-082c5f9630d0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# TODO: Change the model to your own selection\n","name = \"model_2_3\"\n","name = name.lower()\n","resume = '/content/drive/MyDrive/DL_mini_project/checkpoint/model_2_3/053.pth'\n","# resume = f\"./checkpoint/{name}/001.pth\" # change the checkpoint name to the one desired\n","\n","# Model\n","print('==> Building model..')\n","net = ResNet_custom(name).to(device)\n","\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","if resume is not None:\n","    # Load checkpoint.\n","    print(f'==> Resuming from {resume}..')\n","    assert os.path.exists(resume), 'Error: no checkpoint found!'\n","    checkpoint = torch.load(resume)\n","    assert name == checkpoint['name'], 'Error: model does not match checkpoint!'\n","    net.load_state_dict(checkpoint['net'])\n","    best_acc = checkpoint['acc']\n","    start_epoch = checkpoint['epoch']\n","\n","# TODO: Loss\n","criterion = nn.CrossEntropyLoss()\n","# TODO: optimizer, SGD vs Adam, learning rate selection, etc...\n","optimizer = optim.Adam(net.parameters(), lr=0.01)\n","#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n","\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","    train_loss_list.append(train_loss/(batch_idx+1))\n","    train_ACC_list.append(100.*correct/total)\n","\n","\n","    print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","        test_loss_list.append(test_loss/(batch_idx+1))\n","        test_ACC_list.append(100.*correct/total)\n","\n","        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'name': name,\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir(f'checkpoint/{name}'):\n","            os.mkdir(f'checkpoint/{name}')\n","        torch.save(state, f'./checkpoint/{name}/{epoch:03}.pth')\n","        best_acc = acc\n","\n","train_loss_list = []\n","test_loss_list = []\n","train_ACC_list = []\n","test_ACC_list = []\n","lr_list = []\n","\n","# TODO: decide when the training should stop\n","for epoch in range(start_epoch, start_epoch+100):\n","    start_time = time.time()\n","\n","    train(epoch)\n","    test(epoch)\n","\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(\"Learning Rate: %f | Epoch Time: %i m %i s\" % (optimizer.param_groups[0]['lr'], epoch_mins,epoch_secs))\n","\n","    lr_list.append(optimizer.param_groups[0]['lr'])\n","\n","    #scheduler.step()\n","\n","    # break # for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vlh0rZowU4p6"},"outputs":[],"source":["result = pd.DataFrame(np.array([train_loss_list, test_loss_list,\n","                       train_ACC_list, test_ACC_list,\n","                       lr_list]).T, columns=['train_loss','test_loss','train_ACC','test_ACC','lr'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O64qUx0fU4p6"},"outputs":[],"source":["if not os.path.isdir(f'checkpoint/{name}'):\n","    os.mkdir(f'checkpoint/{name}')\n","result.to_csv(f'./checkpoint/{name}/result.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKAWj41kU4p6"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"sSjx83Ad5HEU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LXRhFVio5Hnv"},"source":["# Model_2_4 (Adam Decay lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11289515,"status":"ok","timestamp":1669107749387,"user":{"displayName":"Tao Liang","userId":"03129840086117578976"},"user_tz":300},"outputId":"78265b7b-858a-4552-bd19-5aaf08a4aff6","id":"C9GxdfXa5Hnw"},"outputs":[{"output_type":"stream","name":"stdout","text":["==> Building model..\n","\n","Epoch: 0\n","390 391 Loss: 1.933 | Acc: 27.946% (13973/50000)\n","99 100 Loss: 1.600 | Acc: 39.930% (3993/10000)\n","Saving..\n","Learning Rate: 0.010000 | Epoch Time: 1 m 56 s\n","\n","Epoch: 1\n","390 391 Loss: 1.453 | Acc: 45.950% (22975/50000)\n","99 100 Loss: 1.511 | Acc: 46.210% (4621/10000)\n","Saving..\n","Learning Rate: 0.009998 | Epoch Time: 1 m 53 s\n","\n","Epoch: 2\n","390 391 Loss: 1.164 | Acc: 57.900% (28950/50000)\n","99 100 Loss: 1.118 | Acc: 60.560% (6056/10000)\n","Saving..\n","Learning Rate: 0.009990 | Epoch Time: 1 m 54 s\n","\n","Epoch: 3\n","390 391 Loss: 0.965 | Acc: 65.480% (32740/50000)\n","99 100 Loss: 0.983 | Acc: 65.240% (6524/10000)\n","Saving..\n","Learning Rate: 0.009978 | Epoch Time: 1 m 54 s\n","\n","Epoch: 4\n","390 391 Loss: 0.837 | Acc: 70.326% (35163/50000)\n","99 100 Loss: 0.924 | Acc: 69.020% (6902/10000)\n","Saving..\n","Learning Rate: 0.009961 | Epoch Time: 1 m 54 s\n","\n","Epoch: 5\n","390 391 Loss: 0.735 | Acc: 74.288% (37144/50000)\n","99 100 Loss: 0.929 | Acc: 70.140% (7014/10000)\n","Saving..\n","Learning Rate: 0.009938 | Epoch Time: 1 m 54 s\n","\n","Epoch: 6\n","390 391 Loss: 0.670 | Acc: 76.364% (38182/50000)\n","99 100 Loss: 0.703 | Acc: 75.570% (7557/10000)\n","Saving..\n","Learning Rate: 0.009911 | Epoch Time: 1 m 54 s\n","\n","Epoch: 7\n","390 391 Loss: 0.605 | Acc: 78.890% (39445/50000)\n","99 100 Loss: 0.665 | Acc: 76.280% (7628/10000)\n","Saving..\n","Learning Rate: 0.009880 | Epoch Time: 1 m 53 s\n","\n","Epoch: 8\n","390 391 Loss: 0.560 | Acc: 80.442% (40221/50000)\n","99 100 Loss: 0.744 | Acc: 75.670% (7567/10000)\n","Learning Rate: 0.009843 | Epoch Time: 1 m 53 s\n","\n","Epoch: 9\n","390 391 Loss: 0.510 | Acc: 82.340% (41170/50000)\n","99 100 Loss: 0.541 | Acc: 81.940% (8194/10000)\n","Saving..\n","Learning Rate: 0.009801 | Epoch Time: 1 m 53 s\n","\n","Epoch: 10\n","390 391 Loss: 0.472 | Acc: 83.540% (41770/50000)\n","99 100 Loss: 0.599 | Acc: 79.530% (7953/10000)\n","Learning Rate: 0.009755 | Epoch Time: 1 m 53 s\n","\n","Epoch: 11\n","390 391 Loss: 0.436 | Acc: 84.820% (42410/50000)\n","99 100 Loss: 0.639 | Acc: 79.550% (7955/10000)\n","Learning Rate: 0.009704 | Epoch Time: 1 m 53 s\n","\n","Epoch: 12\n","390 391 Loss: 0.414 | Acc: 85.554% (42777/50000)\n","99 100 Loss: 0.531 | Acc: 82.170% (8217/10000)\n","Saving..\n","Learning Rate: 0.009649 | Epoch Time: 1 m 53 s\n","\n","Epoch: 13\n","390 391 Loss: 0.387 | Acc: 86.416% (43208/50000)\n","99 100 Loss: 0.538 | Acc: 82.300% (8230/10000)\n","Saving..\n","Learning Rate: 0.009589 | Epoch Time: 1 m 53 s\n","\n","Epoch: 14\n","390 391 Loss: 0.360 | Acc: 87.496% (43748/50000)\n","99 100 Loss: 0.508 | Acc: 83.330% (8333/10000)\n","Saving..\n","Learning Rate: 0.009524 | Epoch Time: 1 m 53 s\n","\n","Epoch: 15\n","390 391 Loss: 0.341 | Acc: 88.226% (44113/50000)\n","99 100 Loss: 0.487 | Acc: 84.170% (8417/10000)\n","Saving..\n","Learning Rate: 0.009455 | Epoch Time: 1 m 53 s\n","\n","Epoch: 16\n","390 391 Loss: 0.318 | Acc: 88.802% (44401/50000)\n","99 100 Loss: 0.470 | Acc: 85.150% (8515/10000)\n","Saving..\n","Learning Rate: 0.009382 | Epoch Time: 1 m 53 s\n","\n","Epoch: 17\n","390 391 Loss: 0.294 | Acc: 89.756% (44878/50000)\n","99 100 Loss: 0.496 | Acc: 84.490% (8449/10000)\n","Learning Rate: 0.009304 | Epoch Time: 1 m 53 s\n","\n","Epoch: 18\n","390 391 Loss: 0.278 | Acc: 90.136% (45068/50000)\n","99 100 Loss: 0.444 | Acc: 86.000% (8600/10000)\n","Saving..\n","Learning Rate: 0.009222 | Epoch Time: 1 m 53 s\n","\n","Epoch: 19\n","390 391 Loss: 0.263 | Acc: 90.616% (45308/50000)\n","99 100 Loss: 0.466 | Acc: 85.160% (8516/10000)\n","Learning Rate: 0.009135 | Epoch Time: 1 m 53 s\n","\n","Epoch: 20\n","390 391 Loss: 0.249 | Acc: 91.388% (45694/50000)\n","99 100 Loss: 0.444 | Acc: 86.470% (8647/10000)\n","Saving..\n","Learning Rate: 0.009045 | Epoch Time: 1 m 53 s\n","\n","Epoch: 21\n","390 391 Loss: 0.233 | Acc: 91.888% (45944/50000)\n","99 100 Loss: 0.480 | Acc: 85.360% (8536/10000)\n","Learning Rate: 0.008951 | Epoch Time: 1 m 53 s\n","\n","Epoch: 22\n","390 391 Loss: 0.212 | Acc: 92.376% (46188/50000)\n","99 100 Loss: 0.461 | Acc: 86.500% (8650/10000)\n","Saving..\n","Learning Rate: 0.008853 | Epoch Time: 1 m 53 s\n","\n","Epoch: 23\n","390 391 Loss: 0.205 | Acc: 92.700% (46350/50000)\n","99 100 Loss: 0.519 | Acc: 84.880% (8488/10000)\n","Learning Rate: 0.008751 | Epoch Time: 1 m 53 s\n","\n","Epoch: 24\n","390 391 Loss: 0.191 | Acc: 93.198% (46599/50000)\n","99 100 Loss: 0.471 | Acc: 86.490% (8649/10000)\n","Learning Rate: 0.008645 | Epoch Time: 1 m 52 s\n","\n","Epoch: 25\n","390 391 Loss: 0.175 | Acc: 93.794% (46897/50000)\n","99 100 Loss: 0.450 | Acc: 87.220% (8722/10000)\n","Saving..\n","Learning Rate: 0.008536 | Epoch Time: 1 m 52 s\n","\n","Epoch: 26\n","390 391 Loss: 0.170 | Acc: 93.922% (46961/50000)\n","99 100 Loss: 0.427 | Acc: 87.200% (8720/10000)\n","Learning Rate: 0.008423 | Epoch Time: 1 m 53 s\n","\n","Epoch: 27\n","390 391 Loss: 0.163 | Acc: 94.152% (47076/50000)\n","99 100 Loss: 0.444 | Acc: 87.160% (8716/10000)\n","Learning Rate: 0.008307 | Epoch Time: 1 m 52 s\n","\n","Epoch: 28\n","390 391 Loss: 0.147 | Acc: 94.702% (47351/50000)\n","99 100 Loss: 0.426 | Acc: 88.460% (8846/10000)\n","Saving..\n","Learning Rate: 0.008187 | Epoch Time: 1 m 52 s\n","\n","Epoch: 29\n","390 391 Loss: 0.140 | Acc: 95.040% (47520/50000)\n","99 100 Loss: 0.488 | Acc: 86.830% (8683/10000)\n","Learning Rate: 0.008065 | Epoch Time: 1 m 52 s\n","\n","Epoch: 30\n","390 391 Loss: 0.126 | Acc: 95.568% (47784/50000)\n","99 100 Loss: 0.420 | Acc: 88.070% (8807/10000)\n","Learning Rate: 0.007939 | Epoch Time: 1 m 52 s\n","\n","Epoch: 31\n","390 391 Loss: 0.124 | Acc: 95.562% (47781/50000)\n","99 100 Loss: 0.510 | Acc: 87.570% (8757/10000)\n","Learning Rate: 0.007810 | Epoch Time: 1 m 52 s\n","\n","Epoch: 32\n","390 391 Loss: 0.114 | Acc: 95.978% (47989/50000)\n","99 100 Loss: 0.492 | Acc: 87.750% (8775/10000)\n","Learning Rate: 0.007679 | Epoch Time: 1 m 52 s\n","\n","Epoch: 33\n","390 391 Loss: 0.108 | Acc: 96.018% (48009/50000)\n","99 100 Loss: 0.426 | Acc: 88.650% (8865/10000)\n","Saving..\n","Learning Rate: 0.007545 | Epoch Time: 1 m 52 s\n","\n","Epoch: 34\n","390 391 Loss: 0.099 | Acc: 96.448% (48224/50000)\n","99 100 Loss: 0.441 | Acc: 89.050% (8905/10000)\n","Saving..\n","Learning Rate: 0.007409 | Epoch Time: 1 m 52 s\n","\n","Epoch: 35\n","390 391 Loss: 0.095 | Acc: 96.682% (48341/50000)\n","99 100 Loss: 0.478 | Acc: 88.050% (8805/10000)\n","Learning Rate: 0.007270 | Epoch Time: 1 m 52 s\n","\n","Epoch: 36\n","390 391 Loss: 0.086 | Acc: 96.912% (48456/50000)\n","99 100 Loss: 0.453 | Acc: 89.070% (8907/10000)\n","Saving..\n","Learning Rate: 0.007129 | Epoch Time: 1 m 52 s\n","\n","Epoch: 37\n","390 391 Loss: 0.083 | Acc: 97.048% (48524/50000)\n","99 100 Loss: 0.481 | Acc: 88.490% (8849/10000)\n","Learning Rate: 0.006986 | Epoch Time: 1 m 52 s\n","\n","Epoch: 38\n","390 391 Loss: 0.075 | Acc: 97.250% (48625/50000)\n","99 100 Loss: 0.440 | Acc: 89.830% (8983/10000)\n","Saving..\n","Learning Rate: 0.006841 | Epoch Time: 1 m 52 s\n","\n","Epoch: 39\n","390 391 Loss: 0.074 | Acc: 97.326% (48663/50000)\n","99 100 Loss: 0.493 | Acc: 88.800% (8880/10000)\n","Learning Rate: 0.006694 | Epoch Time: 1 m 52 s\n","\n","Epoch: 40\n","390 391 Loss: 0.068 | Acc: 97.558% (48779/50000)\n","99 100 Loss: 0.511 | Acc: 88.450% (8845/10000)\n","Learning Rate: 0.006545 | Epoch Time: 1 m 52 s\n","\n","Epoch: 41\n","390 391 Loss: 0.066 | Acc: 97.640% (48820/50000)\n","99 100 Loss: 0.558 | Acc: 88.030% (8803/10000)\n","Learning Rate: 0.006395 | Epoch Time: 1 m 52 s\n","\n","Epoch: 42\n","390 391 Loss: 0.058 | Acc: 98.014% (49007/50000)\n","99 100 Loss: 0.525 | Acc: 88.900% (8890/10000)\n","Learning Rate: 0.006243 | Epoch Time: 1 m 52 s\n","\n","Epoch: 43\n","390 391 Loss: 0.056 | Acc: 98.046% (49023/50000)\n","99 100 Loss: 0.569 | Acc: 88.930% (8893/10000)\n","Learning Rate: 0.006091 | Epoch Time: 1 m 52 s\n","\n","Epoch: 44\n","390 391 Loss: 0.057 | Acc: 98.014% (49007/50000)\n","99 100 Loss: 0.516 | Acc: 89.050% (8905/10000)\n","Learning Rate: 0.005937 | Epoch Time: 1 m 52 s\n","\n","Epoch: 45\n","390 391 Loss: 0.049 | Acc: 98.272% (49136/50000)\n","99 100 Loss: 0.505 | Acc: 89.300% (8930/10000)\n","Learning Rate: 0.005782 | Epoch Time: 1 m 52 s\n","\n","Epoch: 46\n","390 391 Loss: 0.048 | Acc: 98.298% (49149/50000)\n","99 100 Loss: 0.507 | Acc: 89.380% (8938/10000)\n","Learning Rate: 0.005627 | Epoch Time: 1 m 52 s\n","\n","Epoch: 47\n","390 391 Loss: 0.040 | Acc: 98.616% (49308/50000)\n","99 100 Loss: 0.561 | Acc: 88.580% (8858/10000)\n","Learning Rate: 0.005471 | Epoch Time: 1 m 52 s\n","\n","Epoch: 48\n","390 391 Loss: 0.042 | Acc: 98.498% (49249/50000)\n","99 100 Loss: 0.529 | Acc: 89.450% (8945/10000)\n","Learning Rate: 0.005314 | Epoch Time: 1 m 52 s\n","\n","Epoch: 49\n","390 391 Loss: 0.041 | Acc: 98.514% (49257/50000)\n","99 100 Loss: 0.545 | Acc: 89.370% (8937/10000)\n","Learning Rate: 0.005157 | Epoch Time: 1 m 52 s\n","\n","Epoch: 50\n","390 391 Loss: 0.035 | Acc: 98.832% (49416/50000)\n","99 100 Loss: 0.604 | Acc: 88.690% (8869/10000)\n","Learning Rate: 0.005000 | Epoch Time: 1 m 52 s\n","\n","Epoch: 51\n","390 391 Loss: 0.034 | Acc: 98.846% (49423/50000)\n","99 100 Loss: 0.551 | Acc: 89.720% (8972/10000)\n","Learning Rate: 0.004843 | Epoch Time: 1 m 52 s\n","\n","Epoch: 52\n","390 391 Loss: 0.032 | Acc: 98.926% (49463/50000)\n","99 100 Loss: 0.539 | Acc: 90.070% (9007/10000)\n","Saving..\n","Learning Rate: 0.004686 | Epoch Time: 1 m 52 s\n","\n","Epoch: 53\n","390 391 Loss: 0.028 | Acc: 99.016% (49508/50000)\n","99 100 Loss: 0.588 | Acc: 89.630% (8963/10000)\n","Learning Rate: 0.004529 | Epoch Time: 1 m 52 s\n","\n","Epoch: 54\n","390 391 Loss: 0.028 | Acc: 99.052% (49526/50000)\n","99 100 Loss: 0.575 | Acc: 89.730% (8973/10000)\n","Learning Rate: 0.004373 | Epoch Time: 1 m 52 s\n","\n","Epoch: 55\n","390 391 Loss: 0.025 | Acc: 99.108% (49554/50000)\n","99 100 Loss: 0.616 | Acc: 88.740% (8874/10000)\n","Learning Rate: 0.004218 | Epoch Time: 1 m 52 s\n","\n","Epoch: 56\n","390 391 Loss: 0.028 | Acc: 99.096% (49548/50000)\n","99 100 Loss: 0.520 | Acc: 90.390% (9039/10000)\n","Saving..\n","Learning Rate: 0.004063 | Epoch Time: 1 m 52 s\n","\n","Epoch: 57\n","390 391 Loss: 0.021 | Acc: 99.240% (49620/50000)\n","99 100 Loss: 0.562 | Acc: 89.870% (8987/10000)\n","Learning Rate: 0.003909 | Epoch Time: 1 m 52 s\n","\n","Epoch: 58\n","390 391 Loss: 0.021 | Acc: 99.320% (49660/50000)\n","99 100 Loss: 0.565 | Acc: 90.100% (9010/10000)\n","Learning Rate: 0.003757 | Epoch Time: 1 m 52 s\n","\n","Epoch: 59\n","390 391 Loss: 0.021 | Acc: 99.254% (49627/50000)\n","99 100 Loss: 0.571 | Acc: 89.560% (8956/10000)\n","Learning Rate: 0.003605 | Epoch Time: 1 m 52 s\n","\n","Epoch: 60\n","390 391 Loss: 0.019 | Acc: 99.346% (49673/50000)\n","99 100 Loss: 0.581 | Acc: 90.000% (9000/10000)\n","Learning Rate: 0.003455 | Epoch Time: 1 m 52 s\n","\n","Epoch: 61\n","390 391 Loss: 0.016 | Acc: 99.422% (49711/50000)\n","99 100 Loss: 0.577 | Acc: 90.190% (9019/10000)\n","Learning Rate: 0.003306 | Epoch Time: 1 m 52 s\n","\n","Epoch: 62\n","390 391 Loss: 0.015 | Acc: 99.486% (49743/50000)\n","99 100 Loss: 0.628 | Acc: 89.280% (8928/10000)\n","Learning Rate: 0.003159 | Epoch Time: 1 m 52 s\n","\n","Epoch: 63\n","390 391 Loss: 0.015 | Acc: 99.494% (49747/50000)\n","99 100 Loss: 0.627 | Acc: 89.870% (8987/10000)\n","Learning Rate: 0.003014 | Epoch Time: 1 m 52 s\n","\n","Epoch: 64\n","390 391 Loss: 0.014 | Acc: 99.532% (49766/50000)\n","99 100 Loss: 0.579 | Acc: 90.560% (9056/10000)\n","Saving..\n","Learning Rate: 0.002871 | Epoch Time: 1 m 52 s\n","\n","Epoch: 65\n","390 391 Loss: 0.011 | Acc: 99.636% (49818/50000)\n","99 100 Loss: 0.591 | Acc: 90.390% (9039/10000)\n","Learning Rate: 0.002730 | Epoch Time: 1 m 52 s\n","\n","Epoch: 66\n","390 391 Loss: 0.012 | Acc: 99.596% (49798/50000)\n","99 100 Loss: 0.574 | Acc: 90.450% (9045/10000)\n","Learning Rate: 0.002591 | Epoch Time: 1 m 52 s\n","\n","Epoch: 67\n","390 391 Loss: 0.010 | Acc: 99.656% (49828/50000)\n","99 100 Loss: 0.613 | Acc: 90.260% (9026/10000)\n","Learning Rate: 0.002455 | Epoch Time: 1 m 52 s\n","\n","Epoch: 68\n","390 391 Loss: 0.009 | Acc: 99.696% (49848/50000)\n","99 100 Loss: 0.589 | Acc: 90.760% (9076/10000)\n","Saving..\n","Learning Rate: 0.002321 | Epoch Time: 1 m 52 s\n","\n","Epoch: 69\n","390 391 Loss: 0.009 | Acc: 99.696% (49848/50000)\n","99 100 Loss: 0.615 | Acc: 90.790% (9079/10000)\n","Saving..\n","Learning Rate: 0.002190 | Epoch Time: 1 m 53 s\n","\n","Epoch: 70\n","390 391 Loss: 0.009 | Acc: 99.722% (49861/50000)\n","99 100 Loss: 0.609 | Acc: 90.630% (9063/10000)\n","Learning Rate: 0.002061 | Epoch Time: 1 m 52 s\n","\n","Epoch: 71\n","390 391 Loss: 0.007 | Acc: 99.744% (49872/50000)\n","99 100 Loss: 0.579 | Acc: 90.740% (9074/10000)\n","Learning Rate: 0.001935 | Epoch Time: 1 m 52 s\n","\n","Epoch: 72\n","390 391 Loss: 0.006 | Acc: 99.794% (49897/50000)\n","99 100 Loss: 0.568 | Acc: 91.110% (9111/10000)\n","Saving..\n","Learning Rate: 0.001813 | Epoch Time: 1 m 52 s\n","\n","Epoch: 73\n","390 391 Loss: 0.005 | Acc: 99.844% (49922/50000)\n","99 100 Loss: 0.576 | Acc: 90.790% (9079/10000)\n","Learning Rate: 0.001693 | Epoch Time: 1 m 52 s\n","\n","Epoch: 74\n","390 391 Loss: 0.005 | Acc: 99.856% (49928/50000)\n","99 100 Loss: 0.578 | Acc: 90.820% (9082/10000)\n","Learning Rate: 0.001577 | Epoch Time: 1 m 52 s\n","\n","Epoch: 75\n","390 391 Loss: 0.005 | Acc: 99.862% (49931/50000)\n","99 100 Loss: 0.576 | Acc: 91.140% (9114/10000)\n","Saving..\n","Learning Rate: 0.001464 | Epoch Time: 1 m 52 s\n","\n","Epoch: 76\n","390 391 Loss: 0.004 | Acc: 99.896% (49948/50000)\n","99 100 Loss: 0.593 | Acc: 91.090% (9109/10000)\n","Learning Rate: 0.001355 | Epoch Time: 1 m 52 s\n","\n","Epoch: 77\n","390 391 Loss: 0.004 | Acc: 99.888% (49944/50000)\n","99 100 Loss: 0.598 | Acc: 91.040% (9104/10000)\n","Learning Rate: 0.001249 | Epoch Time: 1 m 52 s\n","\n","Epoch: 78\n","390 391 Loss: 0.003 | Acc: 99.890% (49945/50000)\n","99 100 Loss: 0.623 | Acc: 90.980% (9098/10000)\n","Learning Rate: 0.001147 | Epoch Time: 1 m 52 s\n","\n","Epoch: 79\n","390 391 Loss: 0.004 | Acc: 99.902% (49951/50000)\n","99 100 Loss: 0.603 | Acc: 91.100% (9110/10000)\n","Learning Rate: 0.001049 | Epoch Time: 1 m 52 s\n","\n","Epoch: 80\n","390 391 Loss: 0.002 | Acc: 99.932% (49966/50000)\n","99 100 Loss: 0.614 | Acc: 90.980% (9098/10000)\n","Learning Rate: 0.000955 | Epoch Time: 1 m 52 s\n","\n","Epoch: 81\n","390 391 Loss: 0.002 | Acc: 99.936% (49968/50000)\n","99 100 Loss: 0.605 | Acc: 91.420% (9142/10000)\n","Saving..\n","Learning Rate: 0.000865 | Epoch Time: 1 m 52 s\n","\n","Epoch: 82\n","390 391 Loss: 0.003 | Acc: 99.932% (49966/50000)\n","99 100 Loss: 0.605 | Acc: 91.200% (9120/10000)\n","Learning Rate: 0.000778 | Epoch Time: 1 m 52 s\n","\n","Epoch: 83\n","390 391 Loss: 0.002 | Acc: 99.932% (49966/50000)\n","99 100 Loss: 0.596 | Acc: 91.310% (9131/10000)\n","Learning Rate: 0.000696 | Epoch Time: 1 m 52 s\n","\n","Epoch: 84\n","390 391 Loss: 0.002 | Acc: 99.924% (49962/50000)\n","99 100 Loss: 0.601 | Acc: 91.350% (9135/10000)\n","Learning Rate: 0.000618 | Epoch Time: 1 m 52 s\n","\n","Epoch: 85\n","390 391 Loss: 0.002 | Acc: 99.942% (49971/50000)\n","99 100 Loss: 0.598 | Acc: 91.260% (9126/10000)\n","Learning Rate: 0.000545 | Epoch Time: 1 m 52 s\n","\n","Epoch: 86\n","390 391 Loss: 0.002 | Acc: 99.958% (49979/50000)\n","99 100 Loss: 0.600 | Acc: 91.290% (9129/10000)\n","Learning Rate: 0.000476 | Epoch Time: 1 m 52 s\n","\n","Epoch: 87\n","390 391 Loss: 0.002 | Acc: 99.964% (49982/50000)\n","99 100 Loss: 0.611 | Acc: 91.330% (9133/10000)\n","Learning Rate: 0.000411 | Epoch Time: 1 m 52 s\n","\n","Epoch: 88\n","390 391 Loss: 0.001 | Acc: 99.978% (49989/50000)\n","99 100 Loss: 0.613 | Acc: 91.150% (9115/10000)\n","Learning Rate: 0.000351 | Epoch Time: 1 m 52 s\n","\n","Epoch: 89\n","390 391 Loss: 0.001 | Acc: 99.976% (49988/50000)\n","99 100 Loss: 0.604 | Acc: 91.300% (9130/10000)\n","Learning Rate: 0.000296 | Epoch Time: 1 m 52 s\n","\n","Epoch: 90\n","390 391 Loss: 0.001 | Acc: 99.984% (49992/50000)\n","99 100 Loss: 0.608 | Acc: 91.180% (9118/10000)\n","Learning Rate: 0.000245 | Epoch Time: 1 m 52 s\n","\n","Epoch: 91\n","390 391 Loss: 0.001 | Acc: 99.974% (49987/50000)\n","99 100 Loss: 0.614 | Acc: 91.030% (9103/10000)\n","Learning Rate: 0.000199 | Epoch Time: 1 m 52 s\n","\n","Epoch: 92\n","390 391 Loss: 0.001 | Acc: 99.978% (49989/50000)\n","99 100 Loss: 0.611 | Acc: 91.240% (9124/10000)\n","Learning Rate: 0.000157 | Epoch Time: 1 m 52 s\n","\n","Epoch: 93\n","390 391 Loss: 0.001 | Acc: 99.966% (49983/50000)\n","99 100 Loss: 0.607 | Acc: 91.140% (9114/10000)\n","Learning Rate: 0.000120 | Epoch Time: 1 m 52 s\n","\n","Epoch: 94\n","390 391 Loss: 0.001 | Acc: 99.984% (49992/50000)\n","99 100 Loss: 0.608 | Acc: 91.150% (9115/10000)\n","Learning Rate: 0.000089 | Epoch Time: 1 m 52 s\n","\n","Epoch: 95\n","390 391 Loss: 0.001 | Acc: 99.966% (49983/50000)\n","99 100 Loss: 0.605 | Acc: 91.240% (9124/10000)\n","Learning Rate: 0.000062 | Epoch Time: 1 m 52 s\n","\n","Epoch: 96\n","390 391 Loss: 0.001 | Acc: 99.974% (49987/50000)\n","99 100 Loss: 0.614 | Acc: 91.130% (9113/10000)\n","Learning Rate: 0.000039 | Epoch Time: 1 m 52 s\n","\n","Epoch: 97\n","390 391 Loss: 0.001 | Acc: 99.978% (49989/50000)\n","99 100 Loss: 0.611 | Acc: 91.270% (9127/10000)\n","Learning Rate: 0.000022 | Epoch Time: 1 m 52 s\n","\n","Epoch: 98\n","390 391 Loss: 0.001 | Acc: 99.982% (49991/50000)\n","99 100 Loss: 0.609 | Acc: 91.240% (9124/10000)\n","Learning Rate: 0.000010 | Epoch Time: 1 m 52 s\n","\n","Epoch: 99\n","390 391 Loss: 0.001 | Acc: 99.976% (49988/50000)\n","99 100 Loss: 0.610 | Acc: 91.330% (9133/10000)\n","Learning Rate: 0.000002 | Epoch Time: 1 m 52 s\n"]}],"source":["# TODO: Change the model to your own selection\n","name = \"model_2_4\"\n","name = name.lower()\n","resume = None\n","# resume = f\"./checkpoint/{name}/001.pth\" # change the checkpoint name to the one desired\n","\n","# Model\n","print('==> Building model..')\n","net = ResNet_custom(name).to(device)\n","\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","if resume is not None:\n","    # Load checkpoint.\n","    print(f'==> Resuming from {resume}..')\n","    assert os.path.exists(resume), 'Error: no checkpoint found!'\n","    checkpoint = torch.load(resume)\n","    assert name == checkpoint['name'], 'Error: model does not match checkpoint!'\n","    net.load_state_dict(checkpoint['net'])\n","    best_acc = checkpoint['acc']\n","    start_epoch = checkpoint['epoch']\n","\n","# TODO: Loss\n","criterion = nn.CrossEntropyLoss()\n","# TODO: optimizer, SGD vs Adam, learning rate selection, etc...\n","optimizer = optim.Adam(net.parameters(), lr=0.01)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n","\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","    train_loss_list.append(train_loss/(batch_idx+1))\n","    train_ACC_list.append(100.*correct/total)\n","\n","\n","    print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","        test_loss_list.append(test_loss/(batch_idx+1))\n","        test_ACC_list.append(100.*correct/total)\n","\n","        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'name': name,\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir(f'checkpoint/{name}'):\n","            os.mkdir(f'checkpoint/{name}')\n","        torch.save(state, f'./checkpoint/{name}/{epoch:03}.pth')\n","        best_acc = acc\n","\n","train_loss_list = []\n","test_loss_list = []\n","train_ACC_list = []\n","test_ACC_list = []\n","lr_list = []\n","\n","# TODO: decide when the training should stop\n","for epoch in range(start_epoch, start_epoch+100):\n","    start_time = time.time()\n","\n","    train(epoch)\n","    test(epoch)\n","\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(\"Learning Rate: %f | Epoch Time: %i m %i s\" % (optimizer.param_groups[0]['lr'], epoch_mins,epoch_secs))\n","\n","    lr_list.append(optimizer.param_groups[0]['lr'])\n","\n","    scheduler.step()\n","\n","    # break # for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rvJDCYZE5Hnx"},"outputs":[],"source":["result = pd.DataFrame(np.array([train_loss_list, test_loss_list,\n","                       train_ACC_list, test_ACC_list,\n","                       lr_list]).T, columns=['train_loss','test_loss','train_ACC','test_ACC','lr'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bB-9J1CQ5Hnx"},"outputs":[],"source":["if not os.path.isdir(f'checkpoint/{name}'):\n","    os.mkdir(f'checkpoint/{name}')\n","result.to_csv(f'./checkpoint/{name}/result.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsBdxydt5Hnx"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["q26mwPqa2W2G","_pyJLJ-32y08","Ix7L5xWH-xJt","5iYiIeEXrG43","sYv75PpXgGb5","Q_LX_RGOi3Kz","_owV7kPJj5CN","YQKT5uUGU4p5","LXRhFVio5Hnv"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}